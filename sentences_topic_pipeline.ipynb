{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccaf7d18",
   "metadata": {},
   "source": [
    "# Discovering Topics from Voted Sentences\n",
    "\n",
    "This notebook walks through a complete pipeline for extracting **topics** from a collection of single-sentence submissions that have been rated by participants. Each sentence can be voted on with one of three outcomes: **agree** (1), **disagree** (-1), or **pass/no opinion** (0). Participants joined at different times and new sentences were added over time, so some sentences were never exposed to earlier participants.  \n",
    "\n",
    "The pipeline addresses several challenges:\n",
    "\n",
    "1. **Semantic deduplication:** multiple sentences may express the same idea; we group paraphrases into a single cluster before analysing votes.\n",
    "2. **Exposure modelling:** because participants see only a subset of sentences (and exposure is not uniform), we model the probability that a participant has seen a sentence and correct for this in the vote-based embeddings.\n",
    "3. **Signed matrix factorization with propensity weighting:** we learn low-dimensional representations of sentences based on participants' votes, using inverse-propensity weights to remove exposure bias and a special treatment for pass votes.\n",
    "4. **Feature fusion and clustering:** we combine vote-derived embeddings with semantic sentence embeddings and cluster them using a density-based algorithm. An agglomerative step builds a hierarchy of topics.\n",
    "5. **Topic labeling and social statistics:** we label clusters with keywords (via c‑TF‑IDF), show representative sentences, and summarise each topic with the distribution of agree/disagree/pass votes.\n",
    "\n",
    "Throughout the notebook we provide commentary to explain each step and choices made. Feel free to adjust hyperparameters, thresholds and weighting schemes to your data and domain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2f4ad9",
   "metadata": {},
   "source": [
    "## Installation (optional)\n",
    "\n",
    "The environment used to run this notebook may already have the necessary libraries installed. If you encounter import errors, uncomment the following lines and run them to install the required packages:\n",
    "\n",
    "```python\n",
    "# !pip install sentence-transformers hdbscan umap-learn faiss-cpu networkx scikit-learn\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5789ce61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T05:50:59.590649Z",
     "iopub.status.busy": "2025-10-09T05:50:59.590384Z",
     "iopub.status.idle": "2025-10-09T05:51:00.031160Z",
     "shell.execute_reply": "2025-10-09T05:51:00.030127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 61 comments and 4538 votes from pol.is export.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(    sentence_id                                               text  \\\n",
       " 6             0       FAccT should be open and welcoming community   \n",
       " 7             1  FAccT should be a generator for alternative so...   \n",
       " 8             2  FAccT is NOT doing well in inviting “non-acade...   \n",
       " 9             3                  FAccT should have poster sessions   \n",
       " 10            4  Facct is not inclusive to the local country/co...   \n",
       " \n",
       "              timestamp  author_id  agrees  disagrees  moderated  \n",
       " 6  2025-06-26 12:52:10          0      87          1          1  \n",
       " 7  2025-06-26 12:53:45          0      42         19          1  \n",
       " 8  2025-06-26 12:53:49          0      37         25          1  \n",
       " 9  2025-06-26 12:53:54          0      61         12          1  \n",
       " 10 2025-06-26 12:53:59          0      22         19          1  ,\n",
       "       participant_id  sentence_id  vote           timestamp\n",
       " 0                  0            0     1 2025-06-29 12:00:08\n",
       " 3108               0           33     1 2025-06-29 12:00:15\n",
       " 2258               0           24     1 2025-06-29 12:00:20\n",
       " 2724               0           29     1 2025-06-29 12:00:35\n",
       " 474                0            5     1 2025-06-29 12:00:41)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Attempt to read the data exported from pol.is. If unavailable, fall back to a synthetic dataset.\n",
    "comments_path = Path(\"comments.csv\")\n",
    "votes_path = Path(\"votes.csv\")\n",
    "\n",
    "if comments_path.exists() and votes_path.exists():\n",
    "    comments_raw = pd.read_csv(comments_path)\n",
    "    votes_raw = pd.read_csv(votes_path)\n",
    "\n",
    "    # Normalise column names and timestamps\n",
    "    sentences_df = comments_raw.rename(\n",
    "        columns={\n",
    "            \"comment-id\": \"sentence_id\",\n",
    "            \"comment-body\": \"text\",\n",
    "            \"author-id\": \"author_id\",\n",
    "        }\n",
    "    ).copy()\n",
    "    sentences_df[\"timestamp\"] = pd.to_datetime(\n",
    "        sentences_df[\"timestamp\"], unit=\"s\", errors=\"coerce\"\n",
    "    )\n",
    "    sentences_df = sentences_df[[\"sentence_id\", \"text\", \"timestamp\", \"author_id\", \"agrees\", \"disagrees\", \"moderated\"]]\n",
    "    sentences_df.sort_values(\"timestamp\", inplace=True)\n",
    "\n",
    "    votes_df = votes_raw.rename(\n",
    "        columns={\n",
    "            \"comment-id\": \"sentence_id\",\n",
    "            \"voter-id\": \"participant_id\",\n",
    "        }\n",
    "    ).copy()\n",
    "    votes_df[\"timestamp\"] = pd.to_datetime(\n",
    "        votes_df[\"timestamp\"], unit=\"s\", errors=\"coerce\"\n",
    "    )\n",
    "    votes_df = votes_df[[\"participant_id\", \"sentence_id\", \"vote\", \"timestamp\"]]\n",
    "    votes_df.sort_values([\"participant_id\", \"timestamp\"], inplace=True)\n",
    "\n",
    "    print(f\"Loaded {len(sentences_df)} comments and {len(votes_df)} votes from pol.is export.\")\n",
    "else:\n",
    "    print(\"Could not load pol.is export; generating a synthetic dataset instead.\")\n",
    "    # Synthetic dataset parameters\n",
    "    num_sentences = 128\n",
    "    num_participants = 60\n",
    "    np.random.seed(0)\n",
    "    # Create synthetic sentences with timestamps spread over 4 months\n",
    "    base_time = np.datetime64(\"2025-01-01\")\n",
    "    times = base_time + np.random.randint(0, 120, size=num_sentences).astype(\"timedelta64[D]\")\n",
    "    sentences_df = pd.DataFrame(\n",
    "        {\n",
    "            \"sentence_id\": np.arange(num_sentences),\n",
    "            \"text\": [f\"Synthetic sentence {i}\" for i in range(num_sentences)],\n",
    "            \"timestamp\": times,\n",
    "        }\n",
    "    )\n",
    "    # Generate participant join times randomly over the same period\n",
    "    join_times = base_time + np.random.randint(0, 120, size=num_participants).astype(\"timedelta64[D]\")\n",
    "    # Synthetic votes: each participant votes on sentences added before they joined, with random agree/disagree/pass\n",
    "    votes_records = []\n",
    "    for p, join_time in enumerate(join_times):\n",
    "        eligible_sentences = sentences_df[sentences_df[\"timestamp\"] <= join_time]\n",
    "        chosen = eligible_sentences.sample(frac=0.5, random_state=p)\n",
    "        for _, row in chosen.iterrows():\n",
    "            vote = np.random.choice([1, -1, 0], p=[0.5, 0.3, 0.2])\n",
    "            votes_records.append(\n",
    "                {\n",
    "                    \"participant_id\": p,\n",
    "                    \"sentence_id\": row[\"sentence_id\"],\n",
    "                    \"vote\": vote,\n",
    "                    \"timestamp\": join_time,\n",
    "                }\n",
    "            )\n",
    "    votes_df = pd.DataFrame(votes_records)\n",
    "    votes_df[\"timestamp\"] = pd.to_datetime(votes_df[\"timestamp\"])\n",
    "    print(\"Synthetic dataset created.\")\n",
    "\n",
    "sentences_df.head(), votes_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e103ac2",
   "metadata": {},
   "source": [
    "## 1. Semantic Deduplication\n",
    "\n",
    "Multiple participants may express the same idea using different words. Before aggregating votes, we group **paraphrases** into single units called *groups*. Our deduplication strategy proceeds in two stages:\n",
    "\n",
    "1. **Bi‑encoder retrieval:** We embed each sentence with a sentence‑level transformer (e.g. `all‑mpnet‑base‑v2`) and use cosine similarity to find candidate paraphrase pairs. Embedding similarity is fast and yields high recall but may include false positives.\n",
    "2. **Cross‑encoder re‑ranking:** We pass each candidate pair through a more accurate cross‑encoder model (e.g. `ms‑marco‑MiniLM‑L‑6‑v2`). This network jointly processes both sentences and predicts a similarity score. We keep pairs above a chosen threshold as paraphrases.\n",
    "\n",
    "The union of verified paraphrase pairs forms a graph; each connected component corresponds to one semantic group.  \n",
    "\n",
    "If the `sentence-transformers` or `networkx` packages are unavailable, this step gracefully degrades by skipping deduplication (each sentence becomes its own group).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f8a618",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T05:51:00.033696Z",
     "iopub.status.busy": "2025-10-09T05:51:00.033459Z",
     "iopub.status.idle": "2025-10-09T05:51:07.384634Z",
     "shell.execute_reply": "2025-10-09T05:51:07.383810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplication complete: 60 groups identified.\n"
     ]
    }
   ],
   "source": [
    "# Attempt to import sentence-transformers. If unavailable, deduplication will be skipped.\n",
    "try:\n",
    "    from sentence_transformers import CrossEncoder, SentenceTransformer, util\n",
    "except ImportError as e:\n",
    "    print(\"sentence-transformers not available:\", e)\n",
    "    SentenceTransformer = None\n",
    "    util = None\n",
    "    CrossEncoder = None\n",
    "\n",
    "# Compute bi‑encoder embeddings if possible\n",
    "bi_encoder = None\n",
    "sent_emb = None\n",
    "if SentenceTransformer is not None:\n",
    "    try:\n",
    "        bi_encoder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "        sent_emb = bi_encoder.encode(\n",
    "            sentences_df[\"text\"].tolist(),\n",
    "            convert_to_tensor=True,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Bi‑encoder unavailable or failed:\", e)\n",
    "        bi_encoder = None\n",
    "        sent_emb = None\n",
    "\n",
    "# Identify candidate paraphrase pairs using embedding similarity\n",
    "paraphrase_pairs = []\n",
    "if sent_emb is not None and util is not None:\n",
    "    candidates = util.paraphrase_mining_embeddings(sent_emb, top_k=20)\n",
    "    COS_THRESHOLD = 0.80\n",
    "    paraphrase_pairs = [(i, j) for score, i, j in candidates if score >= COS_THRESHOLD]\n",
    "\n",
    "# Refine pairs using cross‑encoder\n",
    "verified_pairs = []\n",
    "if paraphrase_pairs:\n",
    "    if CrossEncoder is not None:\n",
    "        try:\n",
    "            ce_model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "            texts = sentences_df[\"text\"].tolist()\n",
    "            pair_texts = [(texts[i], texts[j]) for i, j in paraphrase_pairs]\n",
    "            ce_scores = ce_model.predict(pair_texts)\n",
    "            CE_THRESHOLD = 0.50\n",
    "            verified_pairs = [\n",
    "                (i, j)\n",
    "                for (i, j), s in zip(paraphrase_pairs, ce_scores)\n",
    "                if s >= CE_THRESHOLD\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            print(\"Cross‑encoder unavailable or failed:\", e)\n",
    "            verified_pairs = paraphrase_pairs.copy()\n",
    "    else:\n",
    "        verified_pairs = paraphrase_pairs.copy()\n",
    "\n",
    "# Build connected components of paraphrase graph\n",
    "try:\n",
    "    import networkx as nx\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(len(sentences_df)))\n",
    "    G.add_edges_from(verified_pairs)\n",
    "    components = list(nx.connected_components(G))\n",
    "except Exception as e:\n",
    "    print(\"networkx unavailable or dedup skipped:\", e)\n",
    "    components = [{i} for i in range(len(sentences_df))]\n",
    "\n",
    "# Assign a group_id per sentence\n",
    "group_mapping = {}\n",
    "for cid, comp in enumerate(components):\n",
    "    for idx in comp:\n",
    "        sid = sentences_df.iloc[idx][\"sentence_id\"]\n",
    "        group_mapping[sid] = cid\n",
    "sentences_df[\"group_id\"] = sentences_df[\"sentence_id\"].map(group_mapping)\n",
    "\n",
    "# Merge group_id into votes_df\n",
    "votes_df = votes_df.merge(\n",
    "    sentences_df[[\"sentence_id\", \"group_id\"]], on=\"sentence_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "print(f\"Deduplication complete: {len(components)} groups identified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7417a1aa",
   "metadata": {},
   "source": [
    "## 2. Eligibility and Exposure Propensity\n",
    "\n",
    "Participants joined at various times; sentences were added over time. A participant *cannot* vote on a sentence that was submitted after they left the study. To correct for this **structural missingness**, we build an *eligibility matrix* `E` where `E[i, g] = 1` if participant `i` could have seen sentence group `g`, and `0` otherwise.  \n",
    "\n",
    "Within the eligible pairs there is still **selection bias**: some sentences may be more likely to be shown or noticed than others. We model the probability that an eligible participant votes on a sentence using logistic regression.  \n",
    "\n",
    "Features used in the exposure model:\n",
    "\n",
    "* `sentence_age`: time difference (in days) between the participant’s last vote and the sentence’s submission.\n",
    "* `active_days`: the span (in days) of each participant’s activity (last vote minus first vote).\n",
    "\n",
    "The binary target is 1 if the participant voted on the sentence, 0 otherwise. The resulting predicted probability `p_hat` serves as our **propensity score**. If `scikit-learn` is unavailable, we fall back to a simple ratio: for each group, `p_hat = (# voters)/(# eligible)`.  \n",
    "\n",
    "We will later weight each observed vote by `1 / p_hat` to account for non-uniform exposure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80b3f387",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T05:51:07.387178Z",
     "iopub.status.busy": "2025-10-09T05:51:07.386835Z",
     "iopub.status.idle": "2025-10-09T05:51:09.326579Z",
     "shell.execute_reply": "2025-10-09T05:51:09.325796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exposure model fit with logistic regression.\n",
      "Eligibility sample:\n",
      "     0     1     2     3     4     5     6     7     8     9   ...     50  \\\n",
      "0  True  True  True  True  True  True  True  True  True  True  ...  False   \n",
      "1  True  True  True  True  True  True  True  True  True  True  ...  False   \n",
      "2  True  True  True  True  True  True  True  True  True  True  ...  False   \n",
      "3  True  True  True  True  True  True  True  True  True  True  ...  False   \n",
      "4  True  True  True  True  True  True  True  True  True  True  ...  False   \n",
      "\n",
      "      51     52     53     54     55     56     57     58     59  \n",
      "0  False  False  False  False  False  False  False  False  False  \n",
      "1  False  False  False  False  False  False  False  False  False  \n",
      "2  False  False  False  False  False  False  False  False  False  \n",
      "3  False  False  False  False  False  False  False  False  False  \n",
      "4  False  False  False  False  False  False  False  False  False  \n",
      "\n",
      "[5 rows x 60 columns]\n",
      "\n",
      "Propensity sample:\n",
      "         0         1         2         3         4         5         6   \\\n",
      "0  0.861907  0.861907  0.861907  0.861907  0.861908  0.861908  0.861908   \n",
      "1  0.728455  0.728457  0.728457  0.728457  0.728457  0.728457  0.728457   \n",
      "2  0.728466  0.728467  0.728467  0.728467  0.728467  0.728467  0.728467   \n",
      "3  0.728471  0.728473  0.728473  0.728473  0.728473  0.728473  0.728473   \n",
      "4  0.728448  0.728450  0.728450  0.728450  0.728450  0.728450  0.728450   \n",
      "\n",
      "         7         8         9   ...  50  51  52  53  54  55  56  57  58  59  \n",
      "0  0.861908  0.861908  0.861908  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "1  0.728457  0.728457  0.728457  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "2  0.728468  0.728468  0.728468  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "3  0.728473  0.728473  0.728473  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "4  0.728450  0.728450  0.728450  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "\n",
      "[5 rows x 60 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Compute last and first vote times per participant\n",
    "last_vote_time = votes_df.groupby(\"participant_id\")[\"timestamp\"].max()\n",
    "first_vote_time = votes_df.groupby(\"participant_id\")[\"timestamp\"].min()\n",
    "participant_active_days = (last_vote_time - first_vote_time).dt.total_seconds() / (\n",
    "    24 * 3600.0\n",
    ")\n",
    "\n",
    "# Eligibility matrix: participants x group_id\n",
    "users = votes_df[\"participant_id\"].unique()\n",
    "groups = sentences_df[\"group_id\"].unique()\n",
    "eligibility = pd.DataFrame(False, index=users, columns=groups)\n",
    "\n",
    "# Map group to sentence timestamp\n",
    "group_time_map = sentences_df.set_index(\"group_id\")[\"timestamp\"].to_dict()\n",
    "\n",
    "# Populate eligibility: participant can see group if their last vote is after sentence timestamp\n",
    "for u in users:\n",
    "    last_time = last_vote_time[u]\n",
    "    for g in groups:\n",
    "        if pd.notna(last_time) and pd.notna(group_time_map[g]) and last_time >= group_time_map[g]:\n",
    "            eligibility.loc[u, g] = True\n",
    "\n",
    "# Build data for logistic regression: features and labels for eligible pairs\n",
    "rows = []\n",
    "for u in users:\n",
    "    for g in groups:\n",
    "        if not eligibility.loc[u, g]:\n",
    "            continue\n",
    "        sent_time = group_time_map[g]\n",
    "        lv_time = last_vote_time[u]\n",
    "        sentence_age = (lv_time - sent_time).total_seconds() / (24 * 3600.0)\n",
    "        active_days = participant_active_days[u]\n",
    "        # outcome: 1 if user voted on this group\n",
    "        voted = int(\n",
    "            ((votes_df[\"participant_id\"] == u) & (votes_df[\"group_id\"] == g)).any()\n",
    "        )\n",
    "        rows.append(\n",
    "            {\n",
    "                \"participant_id\": u,\n",
    "                \"group_id\": g,\n",
    "                \"sentence_age\": sentence_age,\n",
    "                \"active_days\": active_days,\n",
    "                \"voted\": voted,\n",
    "            }\n",
    "        )\n",
    "\n",
    "exposure_df = pd.DataFrame(rows)\n",
    "\n",
    "# Normalize features\n",
    "X = exposure_df[[\"sentence_age\", \"active_days\"]].fillna(0.0).values\n",
    "y = exposure_df[\"voted\"].values\n",
    "mean = X.mean(axis=0)\n",
    "std = X.std(axis=0) + 1e-12\n",
    "X_norm = (X - mean) / std\n",
    "\n",
    "# Initialise p_hat DataFrame\n",
    "p_hat = pd.DataFrame(index=users, columns=groups, data=np.nan)\n",
    "\n",
    "try:\n",
    "    # Fit logistic regression\n",
    "    clf = LogisticRegression(max_iter=200)\n",
    "    clf.fit(X_norm, y)\n",
    "    preds = clf.predict_proba(X_norm)[:, 1]\n",
    "    exposure_df[\"probability\"] = preds\n",
    "    # Fill p_hat for eligible pairs\n",
    "    for u, g, p in zip(\n",
    "        exposure_df[\"participant_id\"],\n",
    "        exposure_df[\"group_id\"],\n",
    "        exposure_df[\"probability\"],\n",
    "    ):\n",
    "        p_hat.loc[u, g] = p\n",
    "    print(\"Exposure model fit with logistic regression.\")\n",
    "except Exception as e:\n",
    "    # Fallback: ratio (#voters)/(#eligible) per group\n",
    "    print(\"Logistic regression unavailable, using exposure ratio. Error:\", e)\n",
    "    n_voters = votes_df.groupby(\"group_id\")[\"participant_id\"].nunique()\n",
    "    n_eligible = eligibility.sum(axis=0)\n",
    "    for g in groups:\n",
    "        prob = n_voters.get(g, 0) / max(n_eligible[g], 1)\n",
    "        for u in users:\n",
    "            if eligibility.loc[u, g]:\n",
    "                p_hat.loc[u, g] = prob\n",
    "\n",
    "print(\"Eligibility sample:\")\n",
    "print(eligibility.head())\n",
    "print(\"\\nPropensity sample:\")\n",
    "print(p_hat.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3926e02",
   "metadata": {},
   "source": [
    "## 3. Vote Embedding via Signed Matrix Factorization with Propensity Weighting\n",
    "\n",
    "Our next goal is to represent each sentence group by a low‑dimensional vector capturing how participants voted on it. We use a **signed matrix factorization** model with latent user and group factors (\\(U\\) and \\(V\\)) and biases (\\(b_u\\) and \\(c_v\\)). For each observed vote \\(y_{ij}\\) on group \\(j\\) by user \\(i\\), the score is\n",
    "\n",
    "\\[s_{ij} = U_i \\cdot V_j + b_{u_i} + c_{v_j}\\].\n",
    "\n",
    "We minimise a weighted loss:\n",
    "\n",
    "* **Agree/Disagree (±1):** logistic loss \\(\\log(1+\\exp(-y\\, s))\\).\n",
    "* **Pass (0):** quadratic loss \\(w_0 \\cdot s^2\\) pulling the score toward neutrality, with hyperparameter \\(w_0\\).\n",
    "\n",
    "Each term is weighted by the **inverse propensity** \\(1/p_{ij}\\) computed earlier to correct for selection bias. We also include \\(\\ell_2\\) regularisation on the latent factors. Optimisation is performed with stochastic gradient descent (SGD).  \n",
    "\n",
    "The resulting item factors \\(V\\) serve as our vote‑based embeddings for each sentence group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06522b59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T05:51:09.328662Z",
     "iopub.status.busy": "2025-10-09T05:51:09.328436Z",
     "iopub.status.idle": "2025-10-09T05:51:24.418823Z",
     "shell.execute_reply": "2025-10-09T05:51:24.418037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200, loss = 627.9315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/200, loss = 563.2507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/200, loss = 558.6116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/200, loss = 533.6448\n",
      "First 5 vote embeddings: [[ 1.9783377  -1.11671204 -0.0860506  -0.00721439  0.92986988  4.19952215\n",
      "  -3.05191696  0.27079637 -0.0730158  -0.49096503 -0.20508504 -0.35291271\n",
      "  -2.16140411  0.35052903 -0.66082859 -1.14069716]\n",
      " [ 0.31238407 -0.02368577  1.87799615  1.63209758  0.01706437  0.28531172\n",
      "  -2.26965972 -0.72445035 -0.85762453 -0.76144885  0.47072501 -1.32956924\n",
      "   0.98490974 -0.51816394 -1.73413672 -1.24150323]\n",
      " [ 0.3129001   1.70313543  0.67811289  0.94326179  0.03002561  0.10028031\n",
      "   0.54368785  0.8535412  -2.20133034 -0.1744629   0.70060969  0.33455487\n",
      "   0.30804464  0.28529754  0.7193219  -0.82363672]\n",
      " [-0.2587769  -0.10141546 -2.45206763  0.65560158  0.19394021 -1.02930633\n",
      "  -0.01423074  0.44706764 -0.1201821  -1.24550685  0.27081563 -2.99506991\n",
      "  -1.29544618  1.24493907  1.37783327  1.00407781]\n",
      " [-1.48436691  0.46080736  0.04326834  0.38387733  0.40233809  0.63750902\n",
      "   0.68518626  1.18925226 -0.38998604  0.63534448 -0.16568496  0.20342383\n",
      "   0.37306934 -0.86362053  0.82787336 -1.08835397]]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "k = 16  # latent dimensionality\n",
    "lambda_reg = 1e-4  # regularisation strength\n",
    "w0 = 0.3  # weight for neutral votes\n",
    "epochs = 200\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Map users and groups to indices\n",
    "user_to_idx = {u: idx for idx, u in enumerate(users)}\n",
    "group_to_idx = {g: idx for idx, g in enumerate(groups)}\n",
    "\n",
    "# Build list of (u_idx, g_idx, y, p_hat)\n",
    "train_data = []\n",
    "for _, row in votes_df.iterrows():\n",
    "    u_idx = user_to_idx[row[\"participant_id\"]]\n",
    "    g_idx = group_to_idx[row[\"group_id\"]]\n",
    "    y_val = row[\"vote\"]\n",
    "    p_val = p_hat.loc[row[\"participant_id\"], row[\"group_id\"]]\n",
    "    if p_val and not np.isnan(p_val) and p_val > 0:\n",
    "        train_data.append((u_idx, g_idx, y_val, p_val))\n",
    "\n",
    "# Initialise latent factors and biases\n",
    "num_users = len(users)\n",
    "num_items = len(groups)\n",
    "rng = np.random.default_rng(seed=42)\n",
    "U = 0.1 * rng.standard_normal((num_users, k))\n",
    "V = 0.1 * rng.standard_normal((num_items, k))\n",
    "b_u = np.zeros(num_users)\n",
    "c_v = np.zeros(num_items)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    np.random.shuffle(train_data)\n",
    "    total_loss = 0.0\n",
    "    for u_idx, g_idx, y_val, p_val in train_data:\n",
    "        s = np.dot(U[u_idx], V[g_idx]) + b_u[u_idx] + c_v[g_idx]\n",
    "        if y_val == 1 or y_val == -1:\n",
    "            loss = np.log(1 + np.exp(-y_val * s))\n",
    "            grad_s = -y_val * sigmoid(-y_val * s)\n",
    "        else:\n",
    "            loss = w0 * (s**2)\n",
    "            grad_s = 2 * w0 * s\n",
    "        # Inverse propensity weight\n",
    "        loss *= 1.0 / p_val\n",
    "        grad_s *= 1.0 / p_val\n",
    "        total_loss += loss\n",
    "        # Compute gradients with regularisation\n",
    "        grad_u = grad_s * V[g_idx] + 2 * lambda_reg * U[u_idx]\n",
    "        grad_v = grad_s * U[u_idx] + 2 * lambda_reg * V[g_idx]\n",
    "        # Update parameters\n",
    "        U[u_idx] -= learning_rate * grad_u\n",
    "        V[g_idx] -= learning_rate * grad_v\n",
    "        b_u[u_idx] -= learning_rate * grad_s\n",
    "        c_v[g_idx] -= learning_rate * grad_s\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, loss = {total_loss:.4f}\")\n",
    "\n",
    "vote_embeddings = V.copy()\n",
    "print(\"First 5 vote embeddings:\", vote_embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db75dc0e",
   "metadata": {},
   "source": [
    "## 4. Text Embedding and Feature Fusion\n",
    "\n",
    "While vote embeddings capture participants' opinions, we also want to incorporate **semantic information** from the sentences themselves. We obtain sentence embeddings using the same bi‑encoder as in the deduplication step (if available).  \n",
    "\n",
    "We standardise (zero‑mean/ unit‑variance) both vote and text embeddings and concatenate them. A normalisation to unit length ensures that cosine distance is equivalent to Euclidean distance on the sphere. We then cluster the fused representations using **HDBSCAN**, which automatically determines the number of clusters and handles noise. If `hdbscan` is unavailable, we fall back to `k`‑means.\n",
    "\n",
    "After clustering, we compute centroids of each topic and apply agglomerative clustering to obtain a hierarchy of topics (super‑topics). The labels `topic` and `super_topic` are added to the `sentences_df` DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d1e9d4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T05:51:24.421254Z",
     "iopub.status.busy": "2025-10-09T05:51:24.421032Z",
     "iopub.status.idle": "2025-10-09T05:51:26.292477Z",
     "shell.execute_reply": "2025-10-09T05:51:26.291805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDBSCAN unavailable or insufficient structure, falling back to k-means. Reason: HDBSCAN returned only noise; fallback to k-means.\n",
      "k-means formed 10 clusters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/taking-stock-facct-xWlJi2Mp-py3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/.cache/pypoetry/virtualenvs/taking-stock-facct-xWlJi2Mp-py3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>group_id</th>\n",
       "      <th>topic</th>\n",
       "      <th>super_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentence_id  group_id  topic  super_topic\n",
       "6             0         0      0            5\n",
       "7             1         1      3            4\n",
       "8             2         2      1            8\n",
       "9             3         3      8            1\n",
       "10            4         4      3            4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "\n",
    "## Text embedding\n",
    "text_embeddings = None\n",
    "if \"bi_encoder\" in globals() and bi_encoder is not None:\n",
    "    try:\n",
    "        sentence_embeddings = bi_encoder.encode(\n",
    "            sentences_df[\"text\"].tolist(),\n",
    "            convert_to_tensor=False,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "        group_lookup = {g: idx for idx, g in enumerate(groups)}\n",
    "        group_vectors = np.zeros((len(groups), sentence_embeddings.shape[1]), dtype=float)\n",
    "        counts = np.zeros(len(groups), dtype=int)\n",
    "        for emb, gid in zip(sentence_embeddings, sentences_df[\"group_id\"].tolist()):\n",
    "            idx = group_lookup.get(gid)\n",
    "            if idx is None:\n",
    "                continue\n",
    "            group_vectors[idx] += emb\n",
    "            counts[idx] += 1\n",
    "        nonzero = counts > 0\n",
    "        if np.any(nonzero):\n",
    "            group_vectors[nonzero] /= counts[nonzero, None]\n",
    "        text_embeddings = group_vectors if np.any(counts) else None\n",
    "    except Exception as e:\n",
    "        print(\"Error computing text embeddings:\", e)\n",
    "\n",
    "## Standardise vote and text embeddings\n",
    "vote_scaled = None\n",
    "if \"vote_embeddings\" in globals() and vote_embeddings is not None:\n",
    "    vote_scaler = StandardScaler()\n",
    "    vote_scaled = vote_scaler.fit_transform(vote_embeddings)\n",
    "\n",
    "text_scaled = None\n",
    "if text_embeddings is not None:\n",
    "    text_scaler = StandardScaler()\n",
    "    text_scaled = text_scaler.fit_transform(text_embeddings)\n",
    "\n",
    "# Build feature matrix\n",
    "if vote_scaled is not None and text_scaled is not None:\n",
    "    features = np.hstack([vote_scaled, text_scaled])\n",
    "elif vote_scaled is not None:\n",
    "    features = vote_scaled\n",
    "elif text_scaled is not None:\n",
    "    features = text_scaled\n",
    "else:\n",
    "    raise ValueError(\"No features available for clustering.\")\n",
    "\n",
    "# Normalise to unit norm\n",
    "features_norm = normalize(features)\n",
    "\n",
    "## Clustering\n",
    "cluster_labels = None\n",
    "try:\n",
    "    import hdbscan\n",
    "\n",
    "    hdb = hdbscan.HDBSCAN(min_cluster_size=15, min_samples=5, metric=\"euclidean\")\n",
    "    cluster_labels = hdb.fit_predict(features_norm)\n",
    "    valid = cluster_labels[cluster_labels >= 0]\n",
    "    if valid.size == 0:\n",
    "        raise ValueError(\"HDBSCAN returned only noise; fallback to k-means.\")\n",
    "    print(\"HDBSCAN identified\", len(np.unique(valid)), \"clusters.\")\n",
    "except Exception as e:\n",
    "    print(\"HDBSCAN unavailable or insufficient structure, falling back to k-means. Reason:\", e)\n",
    "    K = min(10, max(2, features_norm.shape[0] // 3))\n",
    "    km = KMeans(n_clusters=K, random_state=42)\n",
    "    cluster_labels = km.fit_predict(features_norm)\n",
    "    print(\"k-means formed\", len(np.unique(cluster_labels)), \"clusters.\")\n",
    "\n",
    "group_cluster_map = {g: int(label) for g, label in zip(groups, cluster_labels)}\n",
    "sentences_df[\"topic\"] = sentences_df[\"group_id\"].map(group_cluster_map)\n",
    "\n",
    "# Compute centroids for each cluster\n",
    "clusters = np.unique(cluster_labels)\n",
    "centroids = np.array(\n",
    "    [features_norm[cluster_labels == c].mean(axis=0) for c in clusters]\n",
    ")\n",
    "\n",
    "# Hierarchical clustering on centroids\n",
    "agg = AgglomerativeClustering(\n",
    "    n_clusters=None, distance_threshold=0.5, metric=\"euclidean\", linkage=\"average\"\n",
    ")\n",
    "hier_labels = agg.fit_predict(centroids)\n",
    "hier_map = {int(c): int(h) for c, h in zip(clusters, hier_labels)}\n",
    "sentences_df[\"super_topic\"] = sentences_df[\"topic\"].map(hier_map)\n",
    "\n",
    "# Persist group-level feature matrix for downstream analysis\n",
    "group_features_norm = features_norm\n",
    "group_ids_ordered = list(groups)\n",
    "\n",
    "sentences_df[[\"sentence_id\", \"group_id\", \"topic\", \"super_topic\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08b0448",
   "metadata": {},
   "source": [
    "## 5. Topic Labeling and Social Statistics\n",
    "\n",
    "To make the discovered topics interpretable, we extract keywords and representative sentences. We employ **class‑based TF‑IDF (c‑TF‑IDF)**: for each topic, we concatenate all sentences in that cluster into a single document and compute TF‑IDF scores over the vocabulary. The top‑scoring n‑grams are selected as keywords.\n",
    "\n",
    "We also present a few sentences closest to the cluster centroid in the fused embedding space.\n",
    "\n",
    "In addition, we summarise the voting behaviour within each topic using the following statistics:\n",
    "\n",
    "* **Coverage:** the proportion of eligible participants who actually voted on sentences in this topic.\n",
    "* **Agree / Disagree / Pass:** the fraction of votes (among exposed participants) that were +1, -1 or 0, respectively.\n",
    "* **Polarity:** the mean vote value (agree = +1, disagree = -1, pass = 0).\n",
    "* **Controversy:** the entropy (base 3) of the agree/pass/disagree distribution; higher values indicate more mixed opinions.\n",
    "\n",
    "These metrics help identify topics with strong consensus versus contentious topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36ac4ace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T05:51:26.294536Z",
     "iopub.status.busy": "2025-10-09T05:51:26.294131Z",
     "iopub.status.idle": "2025-10-09T05:51:29.071606Z",
     "shell.execute_reply": "2025-10-09T05:51:29.070996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "  Keywords: values, doing, facct doing, facct, values facct, values facct doing, community, expression values, respectful exchange interdisciplinary, respectful exchange\n",
      "  Representative sentences:\n",
      "   - FAccT is building a community that shares similar values\n",
      "   - FAccT is doing well in sticking to their values\n",
      "   - FAccT is doing well in selecting diverse locations that are fun and culturally enriched.\n",
      "  Coverage: 88.28%, Agree: 92.0%, Disagree: 1.8%, Pass: 6.2%, Polarity: 0.90, Controversy: 0.29\n",
      "Topic 1\n",
      "  Keywords: workshops, industry, facct, years, panels workshops, panels, 24, ve, ve facct, doing\n",
      "  Representative sentences:\n",
      "   - 3 times I've been to FAccT (18, 23, 24) tech industry influence (e.g. who runs workshops) has been noticeable & sometimes problematic.\n",
      "   - FAccT could require panels & workshops to be designed/led by representatives of that year's region. '24 had decent regional representation.\n",
      "   - In 3 years I've been to FAccT it had plenty of industry input through the papers chosen & who ran or spoke at panels & workshops.\n",
      "  Coverage: 85.94%, Agree: 67.3%, Disagree: 18.2%, Pass: 14.5%, Polarity: 0.49, Controversy: 0.78\n",
      "Topic 2\n",
      "  Keywords: facct, volunteer, volunteer run, academics facct, big, think, minimum, different, run, academics\n",
      "  Representative sentences:\n",
      "   - FAccT should be free for phd students\n",
      "   - FAccT should NOT be sponsored by big corporates\n",
      "   - FAccT should remain volunteer-run (organizers are uncompensated)\n",
      "  Coverage: 91.41%, Agree: 81.2%, Disagree: 9.4%, Pass: 9.4%, Polarity: 0.72, Controversy: 0.56\n",
      "Topic 3\n",
      "  Keywords: visa, facct, local, locations, attendees, meetups eu uk, repeatedly visa requiring, case, case attendees, case attendees fail\n",
      "  Representative sentences:\n",
      "   - FAccT should NOT be repeatedly in visa-requiring locations\n",
      "   - should take visa as a consideration. but \"should not\" sets a barriers for some locations benefiting for hosting FAccT\n",
      "   - Facct should also run local meetups (EU/UK/Asia-Pacific) in case if attendees fail to attend due to visa reasons\n",
      "  Coverage: 92.19%, Agree: 86.4%, Disagree: 2.5%, Pass: 11.0%, Polarity: 0.84, Controversy: 0.42\n",
      "Topic 4\n",
      "  Keywords: faact, conference, future collaborators, useful space, useful space meet, cross, cross disciplinary, cross disciplinary meetings, cs, multidisciplinary conference computer\n",
      "  Representative sentences:\n",
      "   - Faact is mostly focused on a single discipline (CS)\n",
      "   - Faact is a useful space to meet future collaborators\n",
      "   - Faact should support more cross-disciplinary meetings at the conference, not just in the socials.\n",
      "  Coverage: 84.38%, Agree: 83.3%, Disagree: 2.8%, Pass: 13.9%, Polarity: 0.81, Controversy: 0.48\n",
      "Topic 5\n",
      "  Keywords: room, facct, room nap, room facct exclusionary, room facct, exclusionary facct dismiss, exclusionary facct, exclusionary, facct dismiss, facct dismiss environmental\n",
      "  Representative sentences:\n",
      "   - FAccT should NOT be exclusionary\n",
      "   - FAccT should NOT dismiss environmental foot print of research\n",
      "   - FAccT should diversify its epitstemology.\n",
      "  Coverage: 84.38%, Agree: 85.2%, Disagree: 2.8%, Pass: 12.0%, Polarity: 0.82, Controversy: 0.45\n",
      "Topic 6\n",
      "  Keywords: conference, participatory, design, creating, participatory design, facct, facct doing, doing, conference integrates, opportunities participatory\n",
      "  Representative sentences:\n",
      "   - FAccT is not doing well in providing tangible, practical guidance for non-academics to build responsible technology\n",
      "   - FAccT is not doing well in creating opportunities for participatory design, prototyping, and building/creating together AT the conference\n",
      "   - FaCCT should engage in real participatory design\n",
      "  Coverage: 85.94%, Agree: 84.5%, Disagree: 5.5%, Pass: 10.0%, Polarity: 0.79, Controversy: 0.48\n",
      "Topic 7\n",
      "  Keywords: narratives, western, attention, topics, facct, facct inclusive people, comtemplate aware, comtemplate aware relates, naratives, naratives right\n",
      "  Representative sentences:\n",
      "   - FAccT should NOT be centred around western naratives (e.g. US right now) and political atmosphere\n",
      "   - agree on value of neurodiversity, yet, comtemplate \"more aware\" relates to what topics receive less attention,as attention isn't unlimited\n",
      "   - better question is the alternatives (how to) not center around western narratives\n",
      "  Coverage: 79.69%, Agree: 70.6%, Disagree: 10.8%, Pass: 18.6%, Polarity: 0.60, Controversy: 0.73\n",
      "Topic 8\n",
      "  Keywords: sessions, poster, poster sessions, sessions facct, publications, esp, poster sessions facct, facct, need publications justify, start conversation difficult\n",
      "  Representative sentences:\n",
      "   - FAccT should have poster sessions\n",
      "   - the current feedback of poster sessions, esp junior\n",
      "   - Poster sessions provide value as an easy way to randomly talk to anyone and start a conversation, sometimes difficult otherwise at FAccT.\n",
      "  Coverage: 87.50%, Agree: 83.0%, Disagree: 8.0%, Pass: 8.9%, Polarity: 0.75, Controversy: 0.52\n",
      "Topic 9\n",
      "  Keywords: facct, waste facct, waste, plastic, plastic waste, facct vegetarian facct, facct vegetarian, plastic waste facct, facct contribute plastic, facct contribute\n",
      "  Representative sentences:\n",
      "   - FAccT should be vegetarian\n",
      "   - FAccT should NOT contribute to plastic waste\n",
      "   - FAccT should be multilingual\n",
      "  Coverage: 85.94%, Agree: 82.7%, Disagree: 8.2%, Pass: 9.1%, Polarity: 0.75, Controversy: 0.53\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>keywords</th>\n",
       "      <th>representatives</th>\n",
       "      <th>coverage</th>\n",
       "      <th>agree_pct</th>\n",
       "      <th>disagree_pct</th>\n",
       "      <th>pass_pct</th>\n",
       "      <th>polarity</th>\n",
       "      <th>controversy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[values, doing, facct doing, facct, values fac...</td>\n",
       "      <td>[FAccT is building a community that shares sim...</td>\n",
       "      <td>0.882812</td>\n",
       "      <td>0.920354</td>\n",
       "      <td>0.017699</td>\n",
       "      <td>0.061947</td>\n",
       "      <td>0.902655</td>\n",
       "      <td>0.291361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[workshops, industry, facct, years, panels wor...</td>\n",
       "      <td>[3 times I've been to FAccT (18, 23, 24) tech ...</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.672727</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.780124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[facct, volunteer, volunteer run, academics fa...</td>\n",
       "      <td>[FAccT should be free for phd students, FAccT ...</td>\n",
       "      <td>0.914062</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.094017</td>\n",
       "      <td>0.094017</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.558610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[visa, facct, local, locations, attendees, mee...</td>\n",
       "      <td>[FAccT should NOT be repeatedly in visa-requir...</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.110169</td>\n",
       "      <td>0.838983</td>\n",
       "      <td>0.420819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[faact, conference, future collaborators, usef...</td>\n",
       "      <td>[Faact is mostly focused on a single disciplin...</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.478472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[room, facct, room nap, room facct exclusionar...</td>\n",
       "      <td>[FAccT should NOT be exclusionary, FAccT shoul...</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.120370</td>\n",
       "      <td>0.824074</td>\n",
       "      <td>0.446906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[conference, participatory, design, creating, ...</td>\n",
       "      <td>[FAccT is not doing well in providing tangible...</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.845455</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.790909</td>\n",
       "      <td>0.483202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>[narratives, western, attention, topics, facct...</td>\n",
       "      <td>[FAccT should NOT be centred around western na...</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.107843</td>\n",
       "      <td>0.186275</td>\n",
       "      <td>0.598039</td>\n",
       "      <td>0.727353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>[sessions, poster, poster sessions, sessions f...</td>\n",
       "      <td>[FAccT should have poster sessions, the curren...</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.830357</td>\n",
       "      <td>0.080357</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.521268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>[facct, waste facct, waste, plastic, plastic w...</td>\n",
       "      <td>[FAccT should be vegetarian, FAccT should NOT ...</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.827273</td>\n",
       "      <td>0.081818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.745455</td>\n",
       "      <td>0.527639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic                                           keywords  \\\n",
       "0      0  [values, doing, facct doing, facct, values fac...   \n",
       "1      1  [workshops, industry, facct, years, panels wor...   \n",
       "2      2  [facct, volunteer, volunteer run, academics fa...   \n",
       "3      3  [visa, facct, local, locations, attendees, mee...   \n",
       "4      4  [faact, conference, future collaborators, usef...   \n",
       "5      5  [room, facct, room nap, room facct exclusionar...   \n",
       "6      6  [conference, participatory, design, creating, ...   \n",
       "7      7  [narratives, western, attention, topics, facct...   \n",
       "8      8  [sessions, poster, poster sessions, sessions f...   \n",
       "9      9  [facct, waste facct, waste, plastic, plastic w...   \n",
       "\n",
       "                                     representatives  coverage  agree_pct  \\\n",
       "0  [FAccT is building a community that shares sim...  0.882812   0.920354   \n",
       "1  [3 times I've been to FAccT (18, 23, 24) tech ...  0.859375   0.672727   \n",
       "2  [FAccT should be free for phd students, FAccT ...  0.914062   0.811966   \n",
       "3  [FAccT should NOT be repeatedly in visa-requir...  0.921875   0.864407   \n",
       "4  [Faact is mostly focused on a single disciplin...  0.843750   0.833333   \n",
       "5  [FAccT should NOT be exclusionary, FAccT shoul...  0.843750   0.851852   \n",
       "6  [FAccT is not doing well in providing tangible...  0.859375   0.845455   \n",
       "7  [FAccT should NOT be centred around western na...  0.796875   0.705882   \n",
       "8  [FAccT should have poster sessions, the curren...  0.875000   0.830357   \n",
       "9  [FAccT should be vegetarian, FAccT should NOT ...  0.859375   0.827273   \n",
       "\n",
       "   disagree_pct  pass_pct  polarity  controversy  \n",
       "0      0.017699  0.061947  0.902655     0.291361  \n",
       "1      0.181818  0.145455  0.490909     0.780124  \n",
       "2      0.094017  0.094017  0.717949     0.558610  \n",
       "3      0.025424  0.110169  0.838983     0.420819  \n",
       "4      0.027778  0.138889  0.805556     0.478472  \n",
       "5      0.027778  0.120370  0.824074     0.446906  \n",
       "6      0.054545  0.100000  0.790909     0.483202  \n",
       "7      0.107843  0.186275  0.598039     0.727353  \n",
       "8      0.080357  0.089286  0.750000     0.521268  \n",
       "9      0.081818  0.090909  0.745455     0.527639  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def compute_ctfidf(texts, labels, ngram_range=(1, 3), top_k=10):\n",
    "    unique_labels = np.unique(labels)\n",
    "    docs = [' '.join(np.array(texts)[labels == label]) for label in unique_labels]\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(\n",
    "        docs\n",
    "    )\n",
    "    tfidf_matrix = vectorizer.transform(docs)\n",
    "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "    keywords = {}\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        row = tfidf_matrix[i].toarray().flatten()\n",
    "        idx = row.argsort()[::-1][:top_k]\n",
    "        keywords[label] = feature_names[idx].tolist()\n",
    "    return keywords\n",
    "\n",
    "\n",
    "# Keywords per topic\n",
    "keywords_per_topic = compute_ctfidf(\n",
    "    sentences_df[\"text\"].values,\n",
    "    sentences_df[\"topic\"].values,\n",
    "    ngram_range=(1, 3),\n",
    "    top_k=10,\n",
    ")\n",
    "\n",
    "# Representative sentences: nearest group centroids\n",
    "rep_sentences = {}\n",
    "if \"group_ids_ordered\" in globals():\n",
    "    group_ids_array = np.array(group_ids_ordered)\n",
    "    for c in clusters:\n",
    "        group_mask = np.where(cluster_labels == c)[0]\n",
    "        if group_mask.size == 0:\n",
    "            continue\n",
    "        centroid = centroids[np.where(clusters == c)[0][0]]\n",
    "        dists = np.linalg.norm(group_features_norm[group_mask] - centroid, axis=1)\n",
    "        nearest_group_indices = group_mask[np.argsort(dists)[:3]]\n",
    "        nearest_group_ids = group_ids_array[nearest_group_indices]\n",
    "        texts = (\n",
    "            sentences_df[sentences_df[\"group_id\"].isin(nearest_group_ids)]\n",
    "            .sort_values(\"timestamp\")[\"text\"]\n",
    "            .tolist()\n",
    "        )\n",
    "        rep_sentences[c] = texts[:3]\n",
    "else:\n",
    "    rep_sentences = {c: [] for c in clusters}\n",
    "\n",
    "\n",
    "def compute_topic_stats(topic_id):\n",
    "    idxs = sentences_df[sentences_df[\"topic\"] == topic_id].index\n",
    "    groups_in_topic = sentences_df.iloc[idxs][\"group_id\"].unique()\n",
    "    elig_users = 0\n",
    "    voted_users = 0\n",
    "    agree_cnt = 0\n",
    "    disagree_cnt = 0\n",
    "    pass_cnt = 0\n",
    "    for u in users:\n",
    "        eligible_any = False\n",
    "        votes_for_topic = []\n",
    "        for g in groups_in_topic:\n",
    "            if eligibility.loc[u, g]:\n",
    "                eligible_any = True\n",
    "                rows = votes_df[\n",
    "                    (votes_df[\"participant_id\"] == u) & (votes_df[\"group_id\"] == g)\n",
    "                ]\n",
    "                if not rows.empty:\n",
    "                    votes_for_topic.append(rows.iloc[0][\"vote\"])\n",
    "        if eligible_any:\n",
    "            elig_users += 1\n",
    "            if votes_for_topic:\n",
    "                voted_users += 1\n",
    "                # pick strongest vote for this topic\n",
    "                v = sorted(votes_for_topic, key=lambda x: (abs(x), x), reverse=True)[0]\n",
    "                if v == 1:\n",
    "                    agree_cnt += 1\n",
    "                elif v == -1:\n",
    "                    disagree_cnt += 1\n",
    "                else:\n",
    "                    pass_cnt += 1\n",
    "    coverage = voted_users / max(elig_users, 1)\n",
    "    total = agree_cnt + disagree_cnt + pass_cnt\n",
    "    if total > 0:\n",
    "        agree_pct = agree_cnt / total\n",
    "        disagree_pct = disagree_cnt / total\n",
    "        pass_pct = pass_cnt / total\n",
    "        polarity = (agree_cnt - disagree_cnt) / total\n",
    "        probs = np.array([agree_pct, pass_pct, disagree_pct])\n",
    "        entropy = -np.sum(probs * np.log(probs + 1e-12)) / np.log(3)\n",
    "    else:\n",
    "        agree_pct = disagree_pct = pass_pct = polarity = entropy = np.nan\n",
    "    return {\n",
    "        \"coverage\": coverage,\n",
    "        \"agree_pct\": agree_pct,\n",
    "        \"disagree_pct\": disagree_pct,\n",
    "        \"pass_pct\": pass_pct,\n",
    "        \"polarity\": polarity,\n",
    "        \"controversy\": entropy,\n",
    "    }\n",
    "\n",
    "\n",
    "summary_rows = []\n",
    "topic_stats = {c: compute_topic_stats(c) for c in clusters}\n",
    "\n",
    "# Display summary per topic\n",
    "for c in clusters:\n",
    "    print(f\"Topic {c}\")\n",
    "    print(\"  Keywords:\", \", \".join(keywords_per_topic.get(c, [])))\n",
    "    print(\"  Representative sentences:\")\n",
    "    for s in rep_sentences.get(c, []):\n",
    "        print(\"   -\", s)\n",
    "    stats = topic_stats[c]\n",
    "    coverage_txt = f\"{stats['coverage']:.2%}\"\n",
    "    agree_txt = \"nan\" if np.isnan(stats[\"agree_pct\"]) else f\"{stats['agree_pct']:.1%}\"\n",
    "    disagree_txt = \"nan\" if np.isnan(stats[\"disagree_pct\"]) else f\"{stats['disagree_pct']:.1%}\"\n",
    "    pass_txt = \"nan\" if np.isnan(stats[\"pass_pct\"]) else f\"{stats['pass_pct']:.1%}\"\n",
    "    polarity_txt = \"nan\" if np.isnan(stats[\"polarity\"]) else f\"{stats['polarity']:.2f}\"\n",
    "    controversy_txt = \"nan\" if np.isnan(stats[\"controversy\"]) else f\"{stats['controversy']:.2f}\"\n",
    "    print(\n",
    "        f\"  Coverage: {coverage_txt}, Agree: {agree_txt}, Disagree: {disagree_txt}, Pass: {pass_txt}, Polarity: {polarity_txt}, Controversy: {controversy_txt}\"\n",
    "    )\n",
    "    summary_rows.append(\n",
    "        {\n",
    "            \"topic\": int(c),\n",
    "            \"keywords\": keywords_per_topic.get(c, []),\n",
    "            \"representatives\": rep_sentences.get(c, []),\n",
    "            \"coverage\": stats[\"coverage\"],\n",
    "            \"agree_pct\": stats[\"agree_pct\"],\n",
    "            \"disagree_pct\": stats[\"disagree_pct\"],\n",
    "            \"pass_pct\": stats[\"pass_pct\"],\n",
    "            \"polarity\": stats[\"polarity\"],\n",
    "            \"controversy\": stats[\"controversy\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(\"topic\").reset_index(drop=True)\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1950708b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated an end‑to‑end approach to analysing single‑sentence submissions with crowd‑sourced votes. We handled semantic redundancy via paraphrase mining, estimated exposure probabilities to correct for non‑uniform visibility, learned vote‑driven embeddings through signed matrix factorisation weighted by inverse propensities, fused them with semantic sentence embeddings, clustered the fused vectors to find topics, and labelled those topics with keywords and social statistics.\n",
    "\n",
    "The techniques showcased here are modular: you can swap out the embedding models, use alternative exposure models or matrix factorisation algorithms, or experiment with different clustering methods. The general principle remains: **model exposure**, **learn meaningful representations**, **cluster to discover structure**, and **provide interpretable summaries**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (jax)",
   "language": "python",
   "name": "jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
