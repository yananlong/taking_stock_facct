   "source": "# Discovering Topics from Voted Sentences\n\nThis notebook walks through a complete pipeline for extracting **topics** from a collection of single-sentence submissions that have been rated by participants. Each sentence can be voted on with one of three outcomes: **agree** (1), **disagree** (-1), or **pass/no opinion** (0). Participants joined at different times and new sentences were added over time, so some sentences were never exposed to earlier participants.  \n\nThe pipeline addresses several challenges:\n\n1. **Semantic deduplication:** multiple sentences may express the same idea; we group paraphrases into a single cluster before analysing votes.\n2. **Exposure modelling:** because participants see only a subset of sentences (and exposure is not uniform), we model the probability that a participant has seen a sentence and correct for this in the vote-based embeddings.\n3. **Signed matrix factorization with propensity weighting:** we learn low-dimensional representations of sentences based on participants' votes, using inverse-propensity weights to remove exposure bias and a special treatment for pass votes.\n4. **Feature fusion and clustering:** we combine vote-derived embeddings with semantic sentence embeddings and cluster them using a density-based algorithm. An agglomerative step builds a hierarchy of topics.\n5. **Topic labeling and social statistics:** we label clusters with keywords (via c‑TF‑IDF), show representative sentences, and summarise each topic with the distribution of agree/disagree/pass votes.\n\nThroughout the notebook we provide commentary to explain each step and choices made. Feel free to adjust hyperparameters, thresholds and weighting schemes to your data and domain.\n"
   "source": "TODO: Double check all code!!"
   "source": "## Installation (optional)\n\nThe environment used to run this notebook may already have the necessary libraries installed. If you encounter import errors, uncomment the following lines and run them to install the required packages:\n\n```python\n# !pip install sentence-transformers hdbscan umap-learn faiss-cpu networkx scikit-learn\n```\n"
   "source": "from pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import HTML, Markdown, display\nfrom matplotlib import colormaps\nfrom sklearn.inspection import permutation_importance"
     "text": "Loaded 61 comments and 4538 votes from pol.is export.\n"
      "text/plain": "(    sentence_id                                               text  \\\n 6             0       FAccT should be open and welcoming community   \n 7             1  FAccT should be a generator for alternative so...   \n 8             2  FAccT is NOT doing well in inviting “non-acade...   \n 9             3                  FAccT should have poster sessions   \n 10            4  Facct is not inclusive to the local country/co...   \n \n              timestamp  author_id  agrees  disagrees  moderated  \n 6  2025-06-26 12:52:10          0      87          1          1  \n 7  2025-06-26 12:53:45          0      42         19          1  \n 8  2025-06-26 12:53:49          0      37         25          1  \n 9  2025-06-26 12:53:54          0      61         12          1  \n 10 2025-06-26 12:53:59          0      22         19          1  ,\n       participant_id  sentence_id  vote           timestamp\n 0                  0            0     1 2025-06-29 12:00:08\n 3108               0           33     1 2025-06-29 12:00:15\n 2258               0           24     1 2025-06-29 12:00:20\n 2724               0           29     1 2025-06-29 12:00:35\n 474                0            5     1 2025-06-29 12:00:41)"
   "source": "sns.set_theme(style=\"whitegrid\")\nplt.rcParams[\"figure.dpi\"] = 120\n\n# Load the data exported from pol.is. These files are required.\ncomments_path = Path(\"comments.csv\")\nvotes_path = Path(\"votes.csv\")\n\nif not (comments_path.exists() and votes_path.exists()):\n    raise FileNotFoundError(\n        \"comments.csv and votes.csv must be present from the pol.is export.\"\n    )\n\ncomments_raw = pd.read_csv(comments_path)\nvotes_raw = pd.read_csv(votes_path)\n\n# Normalise column names and timestamps\nsentences_df = comments_raw.rename(\n    columns={\n        \"comment-id\": \"sentence_id\",\n        \"comment-body\": \"text\",\n        \"author-id\": \"author_id\",\n    }\n).copy()\nsentences_df[\"timestamp\"] = pd.to_datetime(\n    sentences_df[\"timestamp\"], unit=\"s\", errors=\"coerce\"\n)\nsentences_df = sentences_df[\n    [\n        \"sentence_id\",\n        \"text\",\n        \"timestamp\",\n        \"author_id\",\n        \"agrees\",\n        \"disagrees\",\n        \"moderated\",\n    ]\n]\nsentences_df.sort_values(\"timestamp\", inplace=True)\n\nvotes_df = votes_raw.rename(\n    columns={\n        \"comment-id\": \"sentence_id\",\n        \"voter-id\": \"participant_id\",\n    }\n).copy()\nvotes_df[\"timestamp\"] = pd.to_datetime(\n    votes_df[\"timestamp\"], unit=\"s\", errors=\"coerce\"\n)\nvotes_df = votes_df[[\"participant_id\", \"sentence_id\", \"vote\", \"timestamp\"]]\nvotes_df.sort_values([\"participant_id\", \"timestamp\"], inplace=True)\n\nprint(f\"Loaded {len(sentences_df)} comments and {len(votes_df)} votes from pol.is export.\")\n\nsentences_df.head(), votes_df.head()"
   "source": "## Data Overview\n\nBefore diving into deduplication and modelling, we take a quick look at participation levels and the cadence of new submissions."
     "text": "/tmp/ipykernel_8795/697193905.py:11: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n  .resample(\"H\")\n"
      "text/plain": "<Figure size 1680x1440 with 3 Axes>"
   "source": "from matplotlib.dates import AutoDateLocator, ConciseDateFormatter, HourLocator\n\nfig, axes = plt.subplots(3, 1, figsize=(14, 12))\n\n# Hourly submissions (compressing timeline by removing empty hours)\nif sentences_df[\"timestamp\"].notna().any():\n    # Aggregate by hour and drop hours with zero submissions\n    hourly_timeline = (\n        sentences_df.dropna(subset=[\"timestamp\"])\n        .set_index(\"timestamp\")[\"sentence_id\"]\n        .resample(\"H\")\n        .nunique()\n        .rename(\"unique_sentences_hourly\")\n    )\n    nonzero = hourly_timeline[hourly_timeline > 0]\n    if not nonzero.empty:\n        positions = np.arange(len(nonzero))\n        axes[0].bar(\n            positions,\n            nonzero.values,\n            width=0.8,  # wider bars since empty hours removed\n            color=\"#9467bd\",\n            alpha=0.8,\n            label=\"Hourly new sentences (gaps removed)\",\n        )\n        # Tick every k-th hour to avoid clutter\n        k = max(len(nonzero) // 10, 1)\n        tick_pos = positions[::k]\n        tick_labels = nonzero.index[::k].strftime(\"%Y-%m-%d %H:00\")\n        axes[0].set_xticks(tick_pos)\n        axes[0].set_xticklabels(tick_labels, rotation=45, ha=\"right\")\n        axes[0].set_title(\"Hourly flow of new submissions (empty hours removed)\")\n        axes[0].set_ylabel(\"Count of unique sentences\")\n        axes[0].legend(frameon=False)\n        axes[0].grid(axis=\"y\", alpha=0.3)\n    else:\n        axes[0].axis(\"off\")\n        axes[0].text(\n            0.5,\n            0.5,\n            \"No hourly submissions available.\",\n            ha=\"center\",\n            va=\"center\",\n            fontsize=12,\n        )\nelse:\n    axes[0].axis(\"off\")\n    axes[0].text(\n        0.5,\n        0.5,\n        \"No timestamps available for sentences.\",\n        ha=\"center\",\n        va=\"center\",\n        fontsize=12,\n    )\n\n# Daily submissions + cumulative\nif sentences_df[\"timestamp\"].notna().any():\n    comment_timeline = (\n        sentences_df.dropna(subset=[\"timestamp\"])\n        .set_index(\"timestamp\")[\"sentence_id\"]\n        .resample(\"D\")\n        .nunique()\n        .rename(\"unique_sentences\")\n    )\n    cumulative = comment_timeline.cumsum()\n    axes[1].bar(\n        comment_timeline.index,\n        comment_timeline.values,\n        width=0.8,\n        color=\"#1f77b4\",\n        alpha=0.7,\n        label=\"Daily new sentences\",\n    )\n    axes[1].plot(\n        comment_timeline.index,\n        cumulative.values,\n        color=\"#ff7f0e\",\n        linewidth=2,\n        label=\"Cumulative sentences\",\n    )\n    locator = AutoDateLocator()\n    axes[1].xaxis.set_major_locator(locator)\n    axes[1].xaxis.set_major_formatter(ConciseDateFormatter(locator))\n    axes[1].set_title(\"Daily flow of new submissions\")\n    axes[1].set_ylabel(\"Count of unique sentences\")\n    axes[1].legend(frameon=False)\nelse:\n    axes[1].axis(\"off\")\n    axes[1].text(\n        0.5,\n        0.5,\n        \"No timestamps available for sentences.\",\n        ha=\"center\",\n        va=\"center\",\n        fontsize=12,\n    )\n\n# Votes per participant\nvotes_per_participant = votes_df.groupby(\"participant_id\")[\"vote\"].count()\nif not votes_per_participant.empty:\n    sns.histplot(votes_per_participant, bins=20, color=\"#2ca02c\", ax=axes[2])\n    median_votes = votes_per_participant.median()\n    axes[2].axvline(\n        median_votes,\n        color=\"#d62728\",\n        linestyle=\"--\",\n        label=f\"Median = {median_votes:.0f}\",\n    )\n    axes[2].set_title(\"Votes recorded per participant\")\n    axes[2].set_xlabel(\"Number of votes\")\n    axes[2].set_ylabel(\"Participants\")\n    axes[2].legend(frameon=False)\n    axes[2].grid(axis=\"y\", alpha=0.4)\nelse:\n    axes[2].axis(\"off\")\n    axes[2].text(\n        0.5,\n        0.5,\n        \"No participant vote activity available.\",\n        ha=\"center\",\n        va=\"center\",\n        fontsize=12,\n    )\n\nplt.suptitle(\"Pol.is participation and submission activity overview\", y=1.02)\nplt.tight_layout()\nplt.show()"
   "source": "## 1. Semantic Deduplication\n\nMultiple participants may express the same idea using different words. Before aggregating votes, we group **paraphrases** into single units called *groups*. Our deduplication strategy proceeds in two stages:\n\n1. **Bi‑encoder retrieval:** We embed each sentence with a sentence‑level transformer (e.g. `all‑mpnet‑base‑v2`) and use cosine similarity to find candidate paraphrase pairs. Embedding similarity is fast and yields high recall but may include false positives.\n2. **Cross‑encoder re‑ranking:** We pass each candidate pair through a more accurate cross‑encoder model (e.g. `ms‑marco‑MiniLM‑L‑6‑v2`). This network jointly processes both sentences and predicts a similarity score. We keep pairs above a chosen threshold as paraphrases.\n\nThe union of verified paraphrase pairs forms a graph; each connected component corresponds to one semantic group.  "
   "source": "TODO: Display less similar pairs as well. More general point is that we have robustly checked for duplicates."
     "text": "2025-12-09 15:11:17.701587: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
   "source": "# Import sentence-transformers (required for deduplication)\nfrom sentence_transformers import CrossEncoder, SentenceTransformer, util\n\n"
     "text": "Deduplication complete: 60 groups identified.\n"
   "source": "# Compute bi‑encoder embeddings\nbi_encoder = SentenceTransformer(\"all-mpnet-base-v2\")\nsent_emb = bi_encoder.encode(\n    sentences_df[\"text\"].tolist(),\n    convert_to_tensor=True,\n    normalize_embeddings=True,\n)\n\n# Identify candidate paraphrase pairs using embedding similarity\nparaphrase_pairs = []\ncandidates = util.paraphrase_mining_embeddings(sent_emb, top_k=20)\nCOS_THRESHOLD = 0.9\nparaphrase_pairs = [(i, j) for score, i, j in candidates if score >= COS_THRESHOLD]\n\n# Refine pairs using cross‑encoder\nverified_pairs = []\nif paraphrase_pairs:\n    ce_model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n    texts = sentences_df[\"text\"].tolist()\n    pair_texts = [(texts[i], texts[j]) for i, j in paraphrase_pairs]\n    ce_scores = ce_model.predict(pair_texts)\n    CE_THRESHOLD = 0.50\n    verified_pairs = [\n        (i, j) for (i, j), s in zip(paraphrase_pairs, ce_scores) if s >= CE_THRESHOLD\n    ]\n\n# Build connected components of paraphrase graph\nimport networkx as nx\n\nG = nx.Graph()\nG.add_nodes_from(range(len(sentences_df)))\nG.add_edges_from(verified_pairs)\ncomponents = list(nx.connected_components(G))\n\n# Assign a group_id per sentence\ngroup_mapping = {}\nfor cid, comp in enumerate(components):\n    for idx in comp:\n        sid = sentences_df.iloc[idx][\"sentence_id\"]\n        group_mapping[sid] = cid\nsentences_df[\"group_id\"] = sentences_df[\"sentence_id\"].map(group_mapping)\n\n# Merge group_id into votes_df\nvotes_df = votes_df.merge(\n    sentences_df[[\"sentence_id\", \"group_id\"]], on=\"sentence_id\", how=\"left\",\n)\n\nprint(f\"Deduplication complete: {len(components)} groups identified.\")\n\n"
      "text/plain": "[(20, 59)]"
   "source": "paraphrase_pairs"
     "text": "Found 1 duplicated groups\n            (semantic paraphrase clusters with >1 submission).\n        \n          n_submissions sentence_ids                                                                                                 texts\ngroup_id                                                                                                                                  \n20                    2     [20, 59]  [FAccT should NOT be sponsored by big corporates, FAccT should not get money from big corporations.]\n"
   "source": "dup_counts = (\n    sentences_df.groupby(\"group_id\")[\"sentence_id\"].count().sort_values(ascending=False)\n)\nduplicated_groups = dup_counts[dup_counts > 1].index.tolist()\n\nif duplicated_groups:\n    print(\n        f\"\"\"Found {len(duplicated_groups)} duplicated groups\n            (semantic paraphrase clusters with >1 submission).\n        \"\"\"\n    )\n    duplicates_df = (\n        sentences_df[sentences_df[\"group_id\"].isin(duplicated_groups)]\n        .groupby(\"group_id\")\n        .agg(\n            n_submissions=(\"sentence_id\", \"count\"),\n            sentence_ids=(\"sentence_id\", lambda x: list(x)),\n            texts=(\"text\", lambda x: list(x)),\n        )\n        .sort_values(\"n_submissions\", ascending=False)\n    )\n    print(duplicates_df.to_string())\nelse:\n    print(\"No duplicated submissions detected (each sentence forms its own group).\")"
   "source": "## 2. Eligibility and Exposure Propensity\n\nParticipants joined at various times; sentences were added over time. A participant *cannot* vote on a sentence that was submitted after they left the study. To correct for this **structural missingness**, we build an *eligibility matrix* `E` where `E[i, g] = 1` if participant `i` could have seen sentence group `g`, and `0` otherwise.\n\nWithin the eligible pairs there is still **selection bias**: some sentences may be more likely to be shown or noticed than others. We model the probability that an eligible participant votes on a sentence using logistic regression.\n\nFeatures used in the exposure model:\n\n* `sentence_age`: time difference (in days) between the participant’s last vote and the sentence’s submission.\n* `active_days`: the span (in days) of each participant’s activity (last vote minus first vote).\n\nThe binary target is 1 if the participant voted on the sentence, 0 otherwise. The resulting predicted probability `p_hat` serves as our **propensity score**. If `scikit-learn` is unavailable, we fall back to a simple ratio: for each group, `p_hat = (# voters)/(# eligible)`.\n\nIn the vote-embedding stage we now compare two importance-weighting schemes: `1 / p_hat` retains the original inverse-propensity weighting, while `1 / n_votes` emphasises groups that collected fewer responses. The notebook quantifies both so we can select the approach that reconstructs observed votes more faithfully.\n"
     "text": "Exposure model (Full feature set) fit with logistic regression (ROC-AUC=0.968, AP=0.988, Brier=0.066).\nExposure model (Without participant_unique_groups) fit with logistic regression (ROC-AUC=0.968, AP=0.988, Brier=0.066).\n"
   "source": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import average_precision_score, brier_score_loss, roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\n\n# Compute last and first vote times per participant\nlast_vote_time = votes_df.groupby(\"participant_id\")[\"timestamp\"].max()\nfirst_vote_time = votes_df.groupby(\"participant_id\")[\"timestamp\"].min()\nparticipant_active_days = (last_vote_time - first_vote_time).dt.total_seconds() / (\n    24 * 3600.0\n)\nparticipant_total_votes = votes_df.groupby(\"participant_id\")[\"vote\"].size()\nparticipant_unique_groups = votes_df.groupby(\"participant_id\")[\"group_id\"].nunique()\n\n# Group-level aggregates capture statement visibility in the room\ngroup_total_votes = votes_df.groupby(\"group_id\")[\"vote\"].size()\ngroup_unique_voters = votes_df.groupby(\"group_id\")[\"participant_id\"].nunique()\n\n# Sentence order metadata (used as proxies for discussion recency)\nsentence_order = sentences_df.sort_values(\"timestamp\").reset_index(drop=True)\nsentence_rank_map = {row.group_id: idx + 1 for idx, row in sentence_order.iterrows()}\nsentence_fraction_map = {\n    row.group_id: (idx + 1) / len(sentence_order) if len(sentence_order) else 0.0\n    for idx, row in sentence_order.iterrows()\n}\n\n# Eligibility matrix: participants x group_id\nusers = votes_df[\"participant_id\"].unique()\ngroups = sentences_df[\"group_id\"].unique()\neligibility = pd.DataFrame(False, index=users, columns=groups)\n\n# Map group to sentence timestamp\ngroup_time_map = sentences_df.set_index(\"group_id\")[\"timestamp\"].to_dict()\n\n# Populate eligibility: participant can see group if their last vote is after sentence timestamp\nfor u in users:\n    last_time = last_vote_time[u]\n    for g in groups:\n        if (\n            pd.notna(last_time)\n            and pd.notna(group_time_map[g])\n            and last_time >= group_time_map[g]\n        ):\n            eligibility.loc[u, g] = True\n\n\n# Helper to summarise diagnostic metrics\n\n\ndef summarize_propensity_fit(y_true, y_prob):\n    metrics = {\n        \"n_pairs\": int(len(y_true)),\n        \"n_votes\": int(y_true.sum()),\n    }\n    if len(y_true) == 0:\n        metrics.update(\n            {\"roc_auc\": np.nan, \"average_precision\": np.nan, \"brier\": np.nan}\n        )\n        return metrics\n    clipped = np.clip(y_prob, 1e-6, 1 - 1e-6)\n    metrics[\"brier\"] = float(brier_score_loss(y_true, clipped))\n    if len(np.unique(y_true)) > 1:\n        metrics[\"roc_auc\"] = float(roc_auc_score(y_true, clipped))\n        metrics[\"average_precision\"] = float(average_precision_score(y_true, clipped))\n    else:\n        metrics[\"roc_auc\"] = np.nan\n        metrics[\"average_precision\"] = np.nan\n    return metrics\n\n\n# Build data for logistic regression: features and labels for eligible pairs\nrows = []\nfor u in users:\n    first_time = first_vote_time[u]\n    last_time = last_vote_time[u]\n    participant_total = participant_total_votes.get(u, 0)\n    participant_unique = participant_unique_groups.get(u, 0)\n    active_days = participant_active_days[u]\n    vote_rate = participant_total / max(\n        active_days, 1 / 24\n    )  # votes per day, guard zero span\n    for g in groups:\n        if not eligibility.loc[u, g]:\n            continue\n        sent_time = group_time_map[g]\n        sentence_age = (last_time - sent_time).total_seconds() / (24 * 3600.0)\n        time_since_join = max(\n            (sent_time - first_time).total_seconds() / (24 * 3600.0), 0.0\n        )\n        voted = int(\n            ((votes_df[\"participant_id\"] == u) & (votes_df[\"group_id\"] == g)).any()\n        )\n        rows.append(\n            {\n                \"participant_id\": u,\n                \"group_id\": g,\n                \"sentence_age\": sentence_age,\n                \"active_days\": active_days,\n                \"participant_total_votes\": participant_total,\n                \"participant_unique_groups\": participant_unique,\n                \"participant_vote_rate\": vote_rate,\n                \"time_since_join\": time_since_join,\n                \"group_total_votes\": group_total_votes.get(g, 0),\n                \"group_unique_voters\": group_unique_voters.get(g, 0),\n                \"sentence_rank\": sentence_rank_map.get(g, 0),\n                \"sentence_rank_fraction\": sentence_fraction_map.get(g, 0.0),\n                \"voted\": voted,\n            }\n        )\n\nexposure_df = pd.DataFrame(rows)\nfeature_cols_full = [\n    \"sentence_age\",\n    \"active_days\",\n    \"participant_total_votes\",\n    \"participant_unique_groups\",\n    \"participant_vote_rate\",\n    \"time_since_join\",\n    \"group_total_votes\",\n    \"group_unique_voters\",\n    \"sentence_rank\",\n    \"sentence_rank_fraction\",\n]\npropensity_feature_sets = {\n    \"full\": {\"label\": \"Full feature set\", \"feature_cols\": feature_cols_full},\n    \"no_participant_unique_groups\": {\n        \"label\": \"Without participant_unique_groups\",\n        \"feature_cols\": [\n            col for col in feature_cols_full if col != \"participant_unique_groups\"\n        ],\n    },\n}\n\np_hat = pd.DataFrame(index=users, columns=groups, data=np.nan)\npropensity_models = {}\npropensity_summary_rows = []\npropensity_summary_df = pd.DataFrame()\npropensity_primary_key = None\n\nif not exposure_df.empty:\n    y = exposure_df[\"voted\"].values\n    fallback_probs = None\n\n    for key, meta in propensity_feature_sets.items():\n        subset_cols = meta[\"feature_cols\"]\n        if not subset_cols:\n            print(f\"Skipping {meta['label']} because it has no usable features.\")\n            continue\n        X = exposure_df[subset_cols].fillna(0.0).values\n        scaler = StandardScaler()\n        try:\n            X_norm = scaler.fit_transform(X)\n            clf = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n            clf.fit(X_norm, y)\n            preds = clf.predict_proba(X_norm)[:, 1]\n            fallback = False\n            diagnostics = summarize_propensity_fit(y, preds)\n            print(\n                \"Exposure model (\" + meta[\"label\"] + \") fit with logistic regression \"\n                f\"(ROC-AUC={diagnostics['roc_auc']:.3f}, \"\n                f\"AP={diagnostics['average_precision']:.3f}, \"\n                f\"Brier={diagnostics['brier']:.3f}).\"\n            )\n        except Exception as e:\n            print(\n                f\"Logistic regression unavailable for {meta['label']}; falling back to exposure ratios. Error: {e}\"\n            )\n            if fallback_probs is None:\n                n_voters = votes_df.groupby(\"group_id\")[\"participant_id\"].nunique()\n                n_eligible = eligibility.sum(axis=0)\n                fallback_probs = np.array(\n                    [\n                        n_voters.get(row[\"group_id\"], 0)\n                        / max(n_eligible[row[\"group_id\"]], 1)\n                        for _, row in exposure_df.iterrows()\n                    ]\n                )\n            preds = fallback_probs\n            fallback = True\n            diagnostics = summarize_propensity_fit(y, preds)\n            clf = None\n            scaler = None\n            X_norm = None\n        preds = np.clip(preds, 1e-6, 1 - 1e-6)\n        col_name = f\"probability_{key}\"\n        exposure_df[col_name] = preds\n        propensity_models[key] = {\n            \"label\": meta[\"label\"],\n            \"feature_cols\": subset_cols,\n            \"scaler\": scaler,\n            \"clf\": clf,\n            \"X_norm\": X_norm,\n            \"y\": y,\n            \"probabilities\": preds,\n            \"diagnostics\": diagnostics,\n            \"fallback\": fallback,\n        }\n        propensity_summary_rows.append(\n            {\"model_key\": key, \"model_label\": meta[\"label\"], **diagnostics}\n        )\n        if propensity_primary_key is None:\n            propensity_primary_key = key\n\n    if \"full\" in propensity_models:\n        propensity_primary_key = \"full\"\n\n    if propensity_primary_key is not None:\n        exposure_df = exposure_df.assign(\n            probability=exposure_df[f\"probability_{propensity_primary_key}\"]\n        )\n        for u_val, g_val, p in zip(\n            exposure_df[\"participant_id\"],\n            exposure_df[\"group_id\"],\n            exposure_df[\"probability\"],\n        ):\n            p_hat.loc[u_val, g_val] = p\n        exposure_diagnostics = propensity_models[propensity_primary_key][\"diagnostics\"]\n    else:\n        exposure_diagnostics = summarize_propensity_fit(y, np.zeros_like(y))\nelse:\n    exposure_df = pd.DataFrame(\n        columns=[\n            \"participant_id\",\n            \"group_id\",\n            \"sentence_age\",\n            \"active_days\",\n            \"participant_total_votes\",\n            \"participant_unique_groups\",\n            \"participant_vote_rate\",\n            \"time_since_join\",\n            \"group_total_votes\",\n            \"group_unique_voters\",\n            \"sentence_rank\",\n            \"sentence_rank_fraction\",\n            \"voted\",\n            \"probability\",\n        ]\n    )\n    exposure_diagnostics = {\n        \"n_pairs\": 0,\n        \"n_votes\": 0,\n        \"roc_auc\": np.nan,\n        \"average_precision\": np.nan,\n        \"brier\": np.nan,\n    }\npropensity_summary_df = pd.DataFrame(propensity_summary_rows)"
     "text": "Eligibility sample:\n     0     1     2     3     4     5     6     7     8     9   ...     50  \\\n0  True  True  True  True  True  True  True  True  True  True  ...  False   \n1  True  True  True  True  True  True  True  True  True  True  ...  False   \n2  True  True  True  True  True  True  True  True  True  True  ...  False   \n3  True  True  True  True  True  True  True  True  True  True  ...  False   \n4  True  True  True  True  True  True  True  True  True  True  ...  False   \n\n      51     52     53     54     55     56     57     58     59  \n0  False  False  False  False  False  False  False  False  False  \n1  False  False  False  False  False  False  False  False  False  \n2  False  False  False  False  False  False  False  False  False  \n3  False  False  False  False  False  False  False  False  False  \n4  False  False  False  False  False  False  False  False  False  \n\n[5 rows x 60 columns]\n\nPropensity sample (primary model):\n         0         1         2         3         4         5         6   \\\n0  0.991967  0.991626  0.991103  0.992655  0.989454  0.991864  0.990747   \n1  0.917975  0.914770  0.909883  0.924520  0.894774  0.917011  0.906578   \n2  0.929883  0.927107  0.922867  0.935545  0.909719  0.929048  0.919997   \n3  0.940177  0.937781  0.934119  0.945057  0.922728  0.939457  0.931636   \n4  0.917977  0.914772  0.909885  0.924522  0.894776  0.917013  0.906580   \n\n         7         8         9   ...  50  51  52  53  54  55  56  57  58  59  \n0  0.991663  0.991805  0.993235  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n1  0.915117  0.916447  0.930100  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n2  0.927407  0.928560  0.940362  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n3  0.938041  0.939036  0.949202  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n4  0.915119  0.916449  0.930102  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n\n[5 rows x 60 columns]\n\nPropensity diagnostics (primary model):\nn_pairs              6272.000000\nn_votes              4438.000000\nbrier                   0.065509\nroc_auc                 0.967951\naverage_precision       0.988192\ndtype: float64\n\nPropensity diagnostics by model:\n                   model_key                       model_label  n_pairs  n_votes    brier  roc_auc  average_precision\n                        full                  Full feature set     6272     4438 0.065509 0.967951           0.988192\nno_participant_unique_groups Without participant_unique_groups     6272     4438 0.065641 0.967976           0.988266\n"
   "source": "print(\"Eligibility sample:\")\nprint(eligibility.head())\nprint()\nprint(\"Propensity sample (primary model):\")\nprint(p_hat.head())\nprint()\nprint(\"Propensity diagnostics (primary model):\")\nprint(pd.Series(exposure_diagnostics))\nif not propensity_summary_df.empty:\n    print()\n    print(\"Propensity diagnostics by model:\")\n    print(propensity_summary_df.to_string(index=False))"
   "source": "\n### Feature importance of the exposure propensity model\n\nWe now fit two propensity models: the original full feature set and a baseline that removes `participant_unique_groups`. Both regressions operate on z-scored inputs so coefficient magnitudes remain comparable across predictors. Coefficients and odds ratios capture direction and relative effect size, while permutation importance (using ROC-AUC as the scoring metric) quantifies how much each variable contributes to discrimination when perturbed. Showing both runs side by side highlights how much incremental lift the engagement breadth feature provides.\n"
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>coefficient</th>\n      <th>odds_ratio</th>\n      <th>abs_coefficient</th>\n      <th>model</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>participant_unique_groups</td>\n      <td>2.656875</td>\n      <td>14.251679</td>\n      <td>2.656875</td>\n      <td>Full feature set</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>group_unique_voters</td>\n      <td>1.843706</td>\n      <td>6.319916</td>\n      <td>1.843706</td>\n      <td>Full feature set</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>participant_total_votes</td>\n      <td>1.261903</td>\n      <td>3.532137</td>\n      <td>1.261903</td>\n      <td>Full feature set</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sentence_age</td>\n      <td>-0.834223</td>\n      <td>0.434212</td>\n      <td>0.834223</td>\n      <td>Full feature set</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>participant_vote_rate</td>\n      <td>-0.611461</td>\n      <td>0.542558</td>\n      <td>0.611461</td>\n      <td>Full feature set</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>time_since_join</td>\n      <td>-0.273813</td>\n      <td>0.760474</td>\n      <td>0.273813</td>\n      <td>Full feature set</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>sentence_rank_fraction</td>\n      <td>0.212730</td>\n      <td>1.237050</td>\n      <td>0.212730</td>\n      <td>Full feature set</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>sentence_rank</td>\n      <td>0.212730</td>\n      <td>1.237050</td>\n      <td>0.212730</td>\n      <td>Full feature set</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>group_total_votes</td>\n      <td>-0.202582</td>\n      <td>0.816620</td>\n      <td>0.202582</td>\n      <td>Full feature set</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>active_days</td>\n      <td>-0.045472</td>\n      <td>0.955546</td>\n      <td>0.045472</td>\n      <td>Full feature set</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>participant_total_votes</td>\n      <td>3.493147</td>\n      <td>32.889296</td>\n      <td>3.493147</td>\n      <td>Without participant_unique_groups</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>group_unique_voters</td>\n      <td>1.810956</td>\n      <td>6.116292</td>\n      <td>1.810956</td>\n      <td>Without participant_unique_groups</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>sentence_age</td>\n      <td>-0.844719</td>\n      <td>0.429678</td>\n      <td>0.844719</td>\n      <td>Without participant_unique_groups</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>time_since_join</td>\n      <td>-0.270155</td>\n      <td>0.763261</td>\n      <td>0.270155</td>\n      <td>Without participant_unique_groups</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>sentence_rank_fraction</td>\n      <td>0.208652</td>\n      <td>1.232016</td>\n      <td>0.208652</td>\n      <td>Without participant_unique_groups</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>sentence_rank</td>\n      <td>0.208652</td>\n      <td>1.232016</td>\n      <td>0.208652</td>\n      <td>Without participant_unique_groups</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>group_total_votes</td>\n      <td>-0.166572</td>\n      <td>0.846562</td>\n      <td>0.166572</td>\n      <td>Without participant_unique_groups</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>participant_vote_rate</td>\n      <td>-0.166457</td>\n      <td>0.846659</td>\n      <td>0.166457</td>\n      <td>Without participant_unique_groups</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>active_days</td>\n      <td>0.116760</td>\n      <td>1.123849</td>\n      <td>0.116760</td>\n      <td>Without participant_unique_groups</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                      feature  coefficient  odds_ratio  abs_coefficient  \\\n0   participant_unique_groups     2.656875   14.251679         2.656875   \n1         group_unique_voters     1.843706    6.319916         1.843706   \n2     participant_total_votes     1.261903    3.532137         1.261903   \n3                sentence_age    -0.834223    0.434212         0.834223   \n4       participant_vote_rate    -0.611461    0.542558         0.611461   \n5             time_since_join    -0.273813    0.760474         0.273813   \n6      sentence_rank_fraction     0.212730    1.237050         0.212730   \n7               sentence_rank     0.212730    1.237050         0.212730   \n8           group_total_votes    -0.202582    0.816620         0.202582   \n9                 active_days    -0.045472    0.955546         0.045472   \n10    participant_total_votes     3.493147   32.889296         3.493147   \n11        group_unique_voters     1.810956    6.116292         1.810956   \n12               sentence_age    -0.844719    0.429678         0.844719   \n13            time_since_join    -0.270155    0.763261         0.270155   \n14     sentence_rank_fraction     0.208652    1.232016         0.208652   \n15              sentence_rank     0.208652    1.232016         0.208652   \n16          group_total_votes    -0.166572    0.846562         0.166572   \n17      participant_vote_rate    -0.166457    0.846659         0.166457   \n18                active_days     0.116760    1.123849         0.116760   \n\n                                model  \n0                    Full feature set  \n1                    Full feature set  \n2                    Full feature set  \n3                    Full feature set  \n4                    Full feature set  \n5                    Full feature set  \n6                    Full feature set  \n7                    Full feature set  \n8                    Full feature set  \n9                    Full feature set  \n10  Without participant_unique_groups  \n11  Without participant_unique_groups  \n12  Without participant_unique_groups  \n13  Without participant_unique_groups  \n14  Without participant_unique_groups  \n15  Without participant_unique_groups  \n16  Without participant_unique_groups  \n17  Without participant_unique_groups  \n18  Without participant_unique_groups  "
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>importance_mean</th>\n      <th>importance_std</th>\n      <th>model</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>participant_unique_groups</td>\n      <td>0.303161</td>\n      <td>0.006891</td>\n      <td>Full feature set</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>group_unique_voters</td>\n      <td>0.067820</td>\n      <td>0.002397</td>\n      <td>Full feature set</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>participant_total_votes</td>\n      <td>0.034267</td>\n      <td>0.001744</td>\n      <td>Full feature set</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sentence_age</td>\n      <td>0.009518</td>\n      <td>0.000765</td>\n      <td>Full feature set</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>participant_vote_rate</td>\n      <td>0.001330</td>\n      <td>0.000483</td>\n      <td>Full feature set</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>time_since_join</td>\n      <td>0.001186</td>\n      <td>0.000208</td>\n      <td>Full feature set</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>sentence_rank_fraction</td>\n      <td>0.000712</td>\n      <td>0.000156</td>\n      <td>Full feature set</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>sentence_rank</td>\n      <td>0.000712</td>\n      <td>0.000156</td>\n      <td>Full feature set</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>group_total_votes</td>\n      <td>0.000568</td>\n      <td>0.000128</td>\n      <td>Full feature set</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>active_days</td>\n      <td>0.000023</td>\n      <td>0.000044</td>\n      <td>Full feature set</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>participant_total_votes</td>\n      <td>0.449091</td>\n      <td>0.008143</td>\n      <td>Without participant_unique_groups</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>group_unique_voters</td>\n      <td>0.066502</td>\n      <td>0.002373</td>\n      <td>Without participant_unique_groups</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>sentence_age</td>\n      <td>0.010206</td>\n      <td>0.000788</td>\n      <td>Without participant_unique_groups</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>time_since_join</td>\n      <td>0.001205</td>\n      <td>0.000205</td>\n      <td>Without participant_unique_groups</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>sentence_rank</td>\n      <td>0.000704</td>\n      <td>0.000149</td>\n      <td>Without participant_unique_groups</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>sentence_rank_fraction</td>\n      <td>0.000704</td>\n      <td>0.000149</td>\n      <td>Without participant_unique_groups</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>group_total_votes</td>\n      <td>0.000439</td>\n      <td>0.000100</td>\n      <td>Without participant_unique_groups</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>active_days</td>\n      <td>0.000125</td>\n      <td>0.000130</td>\n      <td>Without participant_unique_groups</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>participant_vote_rate</td>\n      <td>0.000015</td>\n      <td>0.000158</td>\n      <td>Without participant_unique_groups</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                      feature  importance_mean  importance_std  \\\n0   participant_unique_groups         0.303161        0.006891   \n1         group_unique_voters         0.067820        0.002397   \n2     participant_total_votes         0.034267        0.001744   \n3                sentence_age         0.009518        0.000765   \n4       participant_vote_rate         0.001330        0.000483   \n5             time_since_join         0.001186        0.000208   \n6      sentence_rank_fraction         0.000712        0.000156   \n7               sentence_rank         0.000712        0.000156   \n8           group_total_votes         0.000568        0.000128   \n9                 active_days         0.000023        0.000044   \n10    participant_total_votes         0.449091        0.008143   \n11        group_unique_voters         0.066502        0.002373   \n12               sentence_age         0.010206        0.000788   \n13            time_since_join         0.001205        0.000205   \n14              sentence_rank         0.000704        0.000149   \n15     sentence_rank_fraction         0.000704        0.000149   \n16          group_total_votes         0.000439        0.000100   \n17                active_days         0.000125        0.000130   \n18      participant_vote_rate         0.000015        0.000158   \n\n                                model  \n0                    Full feature set  \n1                    Full feature set  \n2                    Full feature set  \n3                    Full feature set  \n4                    Full feature set  \n5                    Full feature set  \n6                    Full feature set  \n7                    Full feature set  \n8                    Full feature set  \n9                    Full feature set  \n10  Without participant_unique_groups  \n11  Without participant_unique_groups  \n12  Without participant_unique_groups  \n13  Without participant_unique_groups  \n14  Without participant_unique_groups  \n15  Without participant_unique_groups  \n16  Without participant_unique_groups  \n17  Without participant_unique_groups  \n18  Without participant_unique_groups  "
     "text": "/tmp/ipykernel_8795/4095753659.py:84: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(\n/tmp/ipykernel_8795/4095753659.py:99: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(\n/tmp/ipykernel_8795/4095753659.py:84: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(\n/tmp/ipykernel_8795/4095753659.py:99: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(\n"
      "text/plain": "<Figure size 2160x1440 with 4 Axes>"
   "source": "feature_importance_df = None\nperm_importance_df = None\nfeature_importance_tables = []\nperm_importance_tables = []\nplot_payload = []\n\nif \"propensity_models\" in globals() and propensity_models and not exposure_df.empty:\n    for key, model in propensity_models.items():\n        label = model.get(\"label\", key)\n        clf = model.get(\"clf\")\n        X_norm = model.get(\"X_norm\")\n        y_vals = model.get(\"y\")\n        feature_cols = model.get(\"feature_cols\", [])\n        if clf is None or X_norm is None or y_vals is None:\n            print(\n                f\"\"\"Skipping feature importance for {label}\n                    because the logistic model was unavailable.\n                \"\"\"\n            )\n            continue\n\n        coef = pd.Series(clf.coef_[0], index=feature_cols)\n        odds = np.exp(coef)\n        coef_df = (\n            pd.DataFrame(\n                {\n                    \"feature\": feature_cols,\n                    \"coefficient\": coef.values,\n                    \"odds_ratio\": odds.values,\n                    \"abs_coefficient\": coef.abs().values,\n                }\n            )\n            .sort_values(\"abs_coefficient\", ascending=False)\n            .reset_index(drop=True)\n        )\n        coef_df[\"model\"] = label\n        feature_importance_tables.append(coef_df)\n\n        perm_result = permutation_importance(\n            clf,\n            X_norm,\n            y_vals,\n            n_repeats=15,\n            random_state=42,\n            scoring=\"roc_auc\",\n            n_jobs=1,\n        )\n        perm_df = (\n            pd.DataFrame(\n                {\n                    \"feature\": feature_cols,\n                    \"importance_mean\": perm_result.importances_mean,\n                    \"importance_std\": perm_result.importances_std,\n                }\n            )\n            .sort_values(\"importance_mean\", ascending=False)\n            .reset_index(drop=True)\n        )\n        perm_df[\"model\"] = label\n        perm_importance_tables.append(perm_df)\n\n        plot_payload.append(\n            (\n                label,\n                coef_df.head(10).sort_values(\"coefficient\"),\n                perm_df.head(10).sort_values(\"importance_mean\"),\n            )\n        )\n\n    if feature_importance_tables:\n        feature_importance_df = pd.concat(feature_importance_tables, ignore_index=True)\n        display(feature_importance_df)\n    if perm_importance_tables:\n        perm_importance_df = pd.concat(perm_importance_tables, ignore_index=True)\n        display(perm_importance_df)\n    if plot_payload:\n        sns.set_context(\"talk\")  # larger base fonts\n        fig, axes = plt.subplots(\n            len(plot_payload), 2, figsize=(18, 6 * len(plot_payload))\n        )\n        if len(plot_payload) == 1:\n            axes = np.array([axes])\n        for row, (label, top_coef, top_perm) in enumerate(plot_payload):\n            sns.barplot(\n                data=top_coef,\n                x=\"coefficient\",\n                y=\"feature\",\n                palette=\"viridis\",\n                ax=axes[row, 0],\n            )\n            axes[row, 0].axvline(0, color=\"black\", linewidth=1, linestyle=\"--\")\n            axes[row, 0].set_title(\n                f\"{label}: coefficients (standardised features)\", fontsize=18\n            )\n            axes[row, 0].set_xlabel(\"Log-odds change per 1 SD increase\", fontsize=16)\n            axes[row, 0].set_ylabel(\"\", fontsize=16)\n            axes[row, 0].tick_params(axis=\"both\", labelsize=14)\n\n            sns.barplot(\n                data=top_perm,\n                x=\"importance_mean\",\n                y=\"feature\",\n                palette=\"magma\",\n                ax=axes[row, 1],\n            )\n            axes[row, 1].set_title(\n                f\"{label}: permutation importances (ROC-AUC drop)\", fontsize=18\n            )\n            axes[row, 1].set_xlabel(\"Mean decrease in ROC-AUC\", fontsize=16)\n            axes[row, 1].set_ylabel(\"\", fontsize=16)\n            axes[row, 1].tick_params(axis=\"both\", labelsize=14)\n\n        plt.tight_layout()\nelse:\n    print(\n        \"Feature importances unavailable because the logistic exposure model was not fit.\"\n    )"
   "source": "\n### Exposure propensity diagnostics\n\nBoth propensity configurations—the full feature set and the baseline without `participant_unique_groups`—blend participant engagement (total votes cast, unique groups, voting rate, and time since joining) with group visibility (ballots, unique voters, chronological rank) and recency features before fitting a class-balanced logistic regression. The printed ROC-AUC, average precision, and Brier score quantify how well each model separates eligible votes from misses, while the per-row plots visualise the probability gap for every specification.\n"
     "text": "/tmp/ipykernel_8795/2959446130.py:65: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(\n/tmp/ipykernel_8795/2959446130.py:65: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(\n"
      "text/plain": "<Figure size 1440x2160 with 6 Axes>"
   "source": "plt.rcParams.update(\n    {\n        \"font.size\": 18,\n        \"axes.titlesize\": 14,\n        \"axes.labelsize\": 14,\n        \"xtick.labelsize\": 12,\n        \"ytick.labelsize\": 12,\n    }\n)\navailable_propensity_plots = []\nif not exposure_df.empty and \"propensity_models\" in globals() and propensity_models:\n    for key, model in propensity_models.items():\n        col_name = f\"probability_{key}\"\n        if col_name not in exposure_df:\n            continue\n        plot_df = exposure_df.dropna(subset=[col_name]).copy()\n        if plot_df.empty:\n            continue\n        plot_df[\"probability\"] = plot_df[col_name]\n        plot_df[\"voted_label\"] = plot_df[\"voted\"].map({0: \"No vote\", 1: \"Voted\"})\n        available_propensity_plots.append((key, model, plot_df))\n\nif available_propensity_plots:\n    palette = {\"No vote\": \"#7f7f7f\", \"Voted\": \"#1f77b4\"}\n\n    n_models = len(available_propensity_plots)\n    fig, axes = plt.subplots(3, n_models, figsize=(6 * n_models, 18), squeeze=False)\n\n    for col_idx, (key, model, plot_df) in enumerate(available_propensity_plots):\n        label = model.get(\"label\", key)\n        metrics = model.get(\"diagnostics\", {})\n        metric_bits = []\n        for display_label, metric_key in [\n            (\"ROC-AUC\", \"roc_auc\"),\n            (\"Avg. precision\", \"average_precision\"),\n            (\"Brier\", \"brier\"),\n        ]:\n            value = metrics.get(metric_key)\n            if value is not None and not pd.isna(value):\n                metric_bits.append(f\"{display_label}: {value:.3f}\")\n        subtitle = \" | \".join(metric_bits)\n        title = f\"{label}: Predicted exposure probability vs. sentence age\"\n        if subtitle:\n            title += \"\\n\" + subtitle\n\n        # Row 0: scatter (smaller circle size)\n        ax0 = axes[0, col_idx]\n        sns.scatterplot(\n            data=plot_df,\n            x=\"sentence_age\",\n            y=\"probability\",\n            hue=\"voted_label\",\n            palette=palette,\n            alpha=0.6,\n            s=20,\n            ax=ax0,\n        )\n        ax0.set_title(title)\n        ax0.set_xlabel(\"Sentence age at last participant activity (days)\")\n        ax0.set_ylabel(\"Predicted probability of exposure\")\n        ax0.legend(frameon=False)\n\n        # Row 1: boxplot (smaller outlier marker size)\n        ax1 = axes[1, col_idx]\n        sns.boxplot(\n            data=plot_df,\n            x=\"voted_label\",\n            y=\"probability\",\n            whis=3,\n            palette=palette,\n            width=0.5,\n            flierprops={\"markersize\": 3},\n            ax=ax1,\n        )\n        ax1.set_title(f\"{label}: Probability distribution by outcome\")\n        ax1.set_xlabel(\"\")\n        ax1.set_ylabel(\"Probability\")\n\n        # Row 2: density\n        ax2 = axes[2, col_idx]\n        sns.kdeplot(\n            data=plot_df,\n            x=\"probability\",\n            hue=\"voted_label\",\n            fill=True,\n            common_norm=False,\n            gridsize=500,\n            alpha=0.4,\n            palette=palette,\n            ax=ax2,\n        )\n        ax2.set_title(f\"{label}: Probability density by outcome\")\n        ax2.set_xlabel(\"Predicted exposure probability\")\n        ax2.set_ylabel(\"Density\")\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"Not enough propensity estimates to visualise.\")"
   "source": "## 3. Vote Embedding via Signed Matrix Factorization with Propensity Weighting\n\nOur next goal is to represent each sentence group by a low‑dimensional vector capturing how participants voted on it. We use a **signed matrix factorization** model with latent user and group factors ($U$ and $V$) and biases ($b_u$ and $c_v$). For each observed vote $y_{ij}$ on group $j$ by user $i$, the score is\n$$s_{ij} = U_i \\cdot V_j + b_{u_i} + c_{v_j}.$$\n\nWe minimise a weighted loss:\n* **Agree/Disagree (±1):** logistic loss $\\log(1+\\exp(-y\\, s))$.\n* **Pass (0):** quadratic loss $w_0 \\cdot s^2$ pulling the score toward neutrality, with hyperparameter $w_0$.\n\nEach term is weighted by the **inverse propensity** $1/p_{ij}$ computed earlier to correct for selection bias. We also include $\\ell_2$ regularisation on the latent factors. Optimisation is performed with stochastic gradient descent (SGD).  \n\nThe resulting item factors $V$ serve as our vote‑based embeddings for each sentence group.\n"
   "source": "def logistic_loss_and_grad(z, y_val):\n    \"\"\"Stable log-loss and gradient for signed votes.\"\"\"\n    if z >= 0:\n        exp_neg_z = np.exp(-z)\n        loss = np.log1p(exp_neg_z)\n        grad = -y_val * exp_neg_z / (1 + exp_neg_z)\n    else:\n        exp_z = np.exp(z)\n        loss = -z + np.log1p(exp_z)\n        grad = -y_val / (1 + exp_z)\n    return loss, grad\n\n\ndef clip_gradients(vec, max_norm):\n    norm = np.linalg.norm(vec)\n    if norm > max_norm and norm > 0:\n        vec *= max_norm / norm\n    return vec\n\n\ndef train_vote_embeddings(train_records, weight_by=\"propensity\", seed=42):\n    \"\"\"Train vote embeddings under a chosen importance-weighting scheme.\"\"\"\n    assert weight_by in {\"propensity\", \"vote_count\"}, \"Unknown weighting\"\n    rng = np.random.default_rng(seed)\n\n    U = 0.1 * rng.standard_normal((num_users, k))\n    V = 0.1 * rng.standard_normal((num_items, k))\n    b_u = np.zeros(num_users)\n    c_v = np.zeros(num_items)\n\n    epoch_losses = []\n    for epoch in range(epochs):\n        order = rng.permutation(len(train_records))\n        total_loss = 0.0\n        for idx in order:\n            record = train_records[idx]\n            u_idx = record[\"u_idx\"]\n            g_idx = record[\"g_idx\"]\n            y_val = record[\"vote\"]\n            weight_value = (\n                record[\"p_hat\"] if weight_by == \"propensity\" else record[\"vote_count\"]\n            )\n            floor = PROPENSITY_FLOOR if weight_by == \"propensity\" else VOTE_COUNT_FLOOR\n            weight_scale = 1.0 / max(weight_value, floor)\n\n            s = np.dot(U[u_idx], V[g_idx]) + b_u[u_idx] + c_v[g_idx]\n            if y_val == 1 or y_val == -1:\n                z = y_val * np.clip(s, -50, 50)\n                loss, grad_s = logistic_loss_and_grad(z, y_val)\n            else:\n                s_clipped = np.clip(s, -10, 10)\n                loss = w0 * (s_clipped**2)\n                grad_s = 2 * w0 * s_clipped\n\n            loss *= weight_scale\n            grad_s *= weight_scale\n            total_loss += loss\n\n            grad_u = grad_s * V[g_idx] + 2 * lambda_reg * U[u_idx]\n            grad_v = grad_s * U[u_idx] + 2 * lambda_reg * V[g_idx]\n            grad_u = clip_gradients(grad_u, MAX_GRAD_NORM)\n            grad_v = clip_gradients(grad_v, MAX_GRAD_NORM)\n            grad_bias = np.clip(grad_s, -MAX_BIAS_GRAD, MAX_BIAS_GRAD)\n\n            U[u_idx] -= learning_rate * grad_u\n            V[g_idx] -= learning_rate * grad_v\n            b_u[u_idx] -= learning_rate * grad_bias\n            c_v[g_idx] -= learning_rate * grad_bias\n\n        epoch_losses.append(total_loss)\n        if (epoch + 1) % 50 == 0:\n            print(\n                f\"{weight_by}: epoch {epoch+1}/{epochs}, weighted loss = {total_loss:.4f}\"\n            )\n\n    return {\n        \"U\": U,\n        \"V\": V,\n        \"b_u\": b_u,\n        \"c_v\": c_v,\n        \"epoch_losses\": epoch_losses,\n    }\n\n\ndef evaluate_embeddings(train_records, model_params):\n    \"\"\"Compute diagnostics that do not re-use the training weights.\"\"\"\n    U = model_params[\"U\"]\n    V = model_params[\"V\"]\n    b_u = model_params[\"b_u\"]\n    c_v = model_params[\"c_v\"]\n\n    sign_hits = 0\n    sign_total = 0\n    neutral_sq = 0.0\n    neutral_total = 0\n    unweighted_loss = 0.0\n\n    for record in train_records:\n        u_idx = record[\"u_idx\"]\n        g_idx = record[\"g_idx\"]\n        y_val = record[\"vote\"]\n        s = np.dot(U[u_idx], V[g_idx]) + b_u[u_idx] + c_v[g_idx]\n\n        if y_val in (1, -1):\n            unweighted_loss += np.log1p(np.exp(-y_val * s))\n            predicted_label = 1 if s >= 0 else -1\n            if predicted_label == y_val:\n                sign_hits += 1\n            sign_total += 1\n        else:\n            unweighted_loss += w0 * (s**2)\n            neutral_sq += s**2\n            neutral_total += 1\n\n    metrics = {\n        \"sign_accuracy\": sign_hits / sign_total if sign_total else np.nan,\n        \"neutral_rmse\": (\n            np.sqrt(neutral_sq / neutral_total) if neutral_total else np.nan\n        ),\n        \"mean_unweighted_loss\": (\n            unweighted_loss / len(train_records) if train_records else np.nan\n        ),\n    }\n    return metrics"
   "source": "# Map users and groups to indices\nuser_to_idx = {u: idx for idx, u in enumerate(users)}\ngroup_to_idx = {g: idx for idx, g in enumerate(groups)}\n\n# Vote counts per group (clip at 1 to avoid divide-by-zero)\ngroup_vote_counts = (\n    votes_df.groupby(\"group_id\")[\"vote\"]\n    .size()\n    .reindex(groups, fill_value=0)\n    .clip(lower=1)\n)\n\n# Build list of training records containing both propensity and vote counts\ntrain_records = []\nfor _, row in votes_df.iterrows():\n    u_idx = user_to_idx[row[\"participant_id\"]]\n    g_idx = group_to_idx[row[\"group_id\"]]\n    y_val = row[\"vote\"]\n    p_val = p_hat.loc[row[\"participant_id\"], row[\"group_id\"]]\n    if pd.notna(p_val) and p_val > 0:\n        train_records.append(\n            {\n                \"u_idx\": u_idx,\n                \"g_idx\": g_idx,\n                \"vote\": y_val,\n                \"p_hat\": float(p_val),\n                \"vote_count\": float(group_vote_counts.loc[row[\"group_id\"]]),\n            }\n        )\n\nnum_users = len(users)\nnum_items = len(groups)\n\nif not train_records:\n    raise ValueError(\"No training records available after propensity filtering.\")"
      "text/plain": "[{'u_idx': 0,\n  'g_idx': 0,\n  'vote': 1,\n  'p_hat': 0.9919665362406039,\n  'vote_count': 97.0},\n {'u_idx': 0,\n  'g_idx': 33,\n  'vote': 1,\n  'p_hat': 0.9961502987530224,\n  'vote_count': 94.0},\n {'u_idx': 0,\n  'g_idx': 24,\n  'vote': 1,\n  'p_hat': 0.9947038408936429,\n  'vote_count': 93.0},\n {'u_idx': 0,\n  'g_idx': 29,\n  'vote': 1,\n  'p_hat': 0.9965764270324924,\n  'vote_count': 98.0},\n {'u_idx': 0,\n  'g_idx': 5,\n  'vote': 1,\n  'p_hat': 0.9918644186795696,\n  'vote_count': 94.0},\n {'u_idx': 0,\n  'g_idx': 22,\n  'vote': 1,\n  'p_hat': 0.9939604410209419,\n  'vote_count': 92.0},\n {'u_idx': 0,\n  'g_idx': 11,\n  'vote': 1,\n  'p_hat': 0.9930671181949589,\n  'vote_count': 94.0},\n {'u_idx': 0,\n  'g_idx': 32,\n  'vote': 0,\n  'p_hat': 0.9960454193578263,\n  'vote_count': 94.0},\n {'u_idx': 0,\n  'g_idx': 31,\n  'vote': -1,\n  'p_hat': 0.9967859021340537,\n  'vote_count': 97.0},\n {'u_idx': 0,\n  'g_idx': 4,\n  'vote': 0,\n  'p_hat': 0.9894538093898823,\n  'vote_count': 91.0},\n {'u_idx': 0,\n  'g_idx': 13,\n  'vote': 1,\n  'p_hat': 0.9947972391584365,\n  'vote_count': 97.0},\n {'u_idx': 0,\n  'g_idx': 8,\n  'vote': 1,\n  'p_hat': 0.9918045996883333,\n  'vote_count': 94.0},\n {'u_idx': 0,\n  'g_idx': 19,\n  'vote': 1,\n  'p_hat': 0.9948204832563173,\n  'vote_count': 95.0},\n {'u_idx': 0,\n  'g_idx': 18,\n  'vote': 1,\n  'p_hat': 0.994249056931898,\n  'vote_count': 94.0},\n {'u_idx': 0,\n  'g_idx': 2,\n  'vote': 1,\n  'p_hat': 0.9911032615025959,\n  'vote_count': 95.0},\n {'u_idx': 0,\n  'g_idx': 15,\n  'vote': 0,\n  'p_hat': 0.9927197143655991,\n  'vote_count': 92.0},\n {'u_idx': 0,\n  'g_idx': 16,\n  'vote': 1,\n  'p_hat': 0.992911322054598,\n  'vote_count': 92.0},\n {'u_idx': 0,\n  'g_idx': 9,\n  'vote': 0,\n  'p_hat': 0.993234670425384,\n  'vote_count': 95.0},\n {'u_idx': 0,\n  'g_idx': 21,\n  'vote': 0,\n  'p_hat': 0.9946920268717209,\n  'vote_count': 94.0},\n {'u_idx': 0,\n  'g_idx': 10,\n  'vote': 1,\n  'p_hat': 0.9939062627649138,\n  'vote_count': 96.0}]"
   "source": "train_records[:20]"
   "source": "# Hyperparameters\nk = 128  # latent dimensionality\nlambda_reg = 1e-4  # regularisation strength\nw0 = 0.3  # weight for neutral votes\nepochs = 500\nlearning_rate = 1e-3\nPROPENSITY_FLOOR = 1e-2  # cap propensity weights at 100x\nVOTE_COUNT_FLOOR = 1.0\nMAX_GRAD_NORM = 10.0\nMAX_BIAS_GRAD = 10.0"
     "text": "propensity: epoch 50/500, weighted loss = 2292.1944\npropensity: epoch 100/500, weighted loss = 1652.5654\npropensity: epoch 150/500, weighted loss = 1338.2302\npropensity: epoch 200/500, weighted loss = 1102.7594\npropensity: epoch 250/500, weighted loss = 913.2467\npropensity: epoch 300/500, weighted loss = 757.1404\npropensity: epoch 350/500, weighted loss = 627.4320\npropensity: epoch 400/500, weighted loss = 520.3722\npropensity: epoch 450/500, weighted loss = 433.1315\npropensity: epoch 500/500, weighted loss = 362.8066\nvote_count: epoch 50/500, weighted loss = 26.5771\nvote_count: epoch 100/500, weighted loss = 26.2943\nvote_count: epoch 150/500, weighted loss = 26.0279\nvote_count: epoch 200/500, weighted loss = 25.7767\nvote_count: epoch 250/500, weighted loss = 25.5394\nvote_count: epoch 300/500, weighted loss = 25.3149\nvote_count: epoch 350/500, weighted loss = 25.1024\nvote_count: epoch 400/500, weighted loss = 24.9008\nvote_count: epoch 450/500, weighted loss = 24.7095\nvote_count: epoch 500/500, weighted loss = 24.5276\n"
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>weighting</th>\n      <th>final_weighted_loss</th>\n      <th>sign_accuracy</th>\n      <th>neutral_rmse</th>\n      <th>mean_unweighted_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Inverse propensity</td>\n      <td>362.806559</td>\n      <td>0.999311</td>\n      <td>0.191741</td>\n      <td>0.074239</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Inverse total votes</td>\n      <td>24.527634</td>\n      <td>0.746556</td>\n      <td>0.163569</td>\n      <td>0.423397</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "             weighting  final_weighted_loss  sign_accuracy  neutral_rmse  \\\n0   Inverse propensity           362.806559       0.999311      0.191741   \n1  Inverse total votes            24.527634       0.746556      0.163569   \n\n   mean_unweighted_loss  \n0              0.074239  \n1              0.423397  "
     "text": "Selected weighting: Inverse propensity (min mean unweighted loss)\n"
   "source": "models = {}\ncomparison_rows = []\nfor label, key, seed in [\n    (\"Inverse propensity\", \"propensity\", 42),\n    (\"Inverse total votes\", \"vote_count\", 84),\n]:\n    model = train_vote_embeddings(train_records, weight_by=key, seed=seed)\n    diagnostics = evaluate_embeddings(train_records, model)\n    models[label] = {**model, \"metrics\": diagnostics, \"weight_key\": key}\n    comparison_rows.append(\n        {\n            \"weighting\": label,\n            \"final_weighted_loss\": model[\"epoch_losses\"][-1],\n            \"sign_accuracy\": diagnostics[\"sign_accuracy\"],\n            \"neutral_rmse\": diagnostics[\"neutral_rmse\"],\n            \"mean_unweighted_loss\": diagnostics[\"mean_unweighted_loss\"],\n        }\n    )\n\ncomparison_df = pd.DataFrame(comparison_rows)\ndisplay(comparison_df)\n\nbest_row = comparison_df.sort_values(\"mean_unweighted_loss\").iloc[0]\nbest_label = best_row[\"weighting\"]\nprint(f\"Selected weighting: {best_label} (min mean unweighted loss)\")\n\nbest_model = models[best_label]\nU = best_model[\"U\"]\nV = best_model[\"V\"]\nb_u = best_model[\"b_u\"]\nc_v = best_model[\"c_v\"]\nvote_weighting_strategy = best_label\nvote_embeddings = V.copy()"
   "source": "The table above compares the inverse-propensity baseline against weighting by the raw vote count. We pick the scheme with the lower unweighted reconstruction loss to carry forward in the analysis (recorded in `vote_weighting_strategy`)."
   "source": "## 4. Text Embedding and Feature Fusion\n\nWhile vote embeddings capture participants' opinions, we also want to incorporate **semantic information** from the sentences themselves. We obtain sentence embeddings using the same bi‑encoder as in the deduplication step (if available).  \n\nWe standardise (zero‑mean/ unit‑variance) both vote and text embeddings and concatenate them. A normalisation to unit length ensures that cosine distance is equivalent to Euclidean distance on the sphere. We then cluster the fused representations using **HDBSCAN**, which automatically determines the number of clusters and handles noise. If `hdbscan` is unavailable, we fall back to `k`‑means.\n\nAfter clustering, we compute centroids of each topic and apply agglomerative clustering to obtain a hierarchy of topics (super‑topics). The labels `topic` and `super_topic` are added to the `sentences_df` DataFrame.\n"
   "source": "TODO: do the same analysis without voting patterns"
     "text": "HDBSCAN unavailable or insufficient structure, falling back to k-means. Reason: No module named 'hdbscan'\nk-means formed 10 clusters.\n"
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_id</th>\n      <th>group_id</th>\n      <th>topic</th>\n      <th>super_topic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3</td>\n      <td>3</td>\n      <td>8</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>4</td>\n      <td>4</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "    sentence_id  group_id  topic  super_topic\n6             0         0      0            5\n7             1         1      3            4\n8             2         2      1            8\n9             3         3      8            1\n10            4         4      3            4"
   "source": "from sklearn.cluster import AgglomerativeClustering, KMeans\nfrom sklearn.preprocessing import StandardScaler, normalize\n\n## Text embedding\nsentence_embeddings = bi_encoder.encode(\n    sentences_df[\"text\"].tolist(),\n    convert_to_tensor=False,\n    normalize_embeddings=True,\n)\ngroup_lookup = {g: idx for idx, g in enumerate(groups)}\ngroup_vectors = np.zeros((len(groups), sentence_embeddings.shape[1]), dtype=float)\ncounts = np.zeros(len(groups), dtype=int)\nfor emb, gid in zip(sentence_embeddings, sentences_df[\"group_id\"].tolist()):\n    idx = group_lookup.get(gid)\n    if idx is None:\n        continue\n    group_vectors[idx] += emb\n    counts[idx] += 1\nnonzero = counts > 0\nif np.any(nonzero):\n    group_vectors[nonzero] /= counts[nonzero, None]\ntext_embeddings = group_vectors if np.any(counts) else None\n\n## Standardise vote and text embeddings\nvote_scaled = None\nif \"vote_embeddings\" in globals() and vote_embeddings is not None:\n    vote_scaler = StandardScaler()\n    vote_scaled = vote_scaler.fit_transform(vote_embeddings)\n\ntext_scaled = None\nif text_embeddings is not None:\n    text_scaler = StandardScaler()\n    text_scaled = text_scaler.fit_transform(text_embeddings)\n\n# Build feature matrix\nif vote_scaled is not None and text_scaled is not None:\n    features = np.hstack([vote_scaled, text_scaled])\nelif vote_scaled is not None:\n    features = vote_scaled\nelif text_scaled is not None:\n    features = text_scaled\nelse:\n    raise ValueError(\"No features available for clustering.\")\n\n# Normalise to unit norm\nfeatures_norm = normalize(features)\n\n## Clustering\ncluster_method = \"hdbscan\"  # Options: \"hdbscan\" or \"kmeans\"\nif cluster_method == \"hdbscan\":\n    import hdbscan\n\n    hdb = hdbscan.HDBSCAN(min_cluster_size=15, min_samples=5, metric=\"euclidean\")\n    cluster_labels = hdb.fit_predict(features_norm)\n    valid = cluster_labels[cluster_labels >= 0]\n    if valid.size == 0:\n        raise ValueError(\"HDBSCAN returned only noise.\")\n    print(\"HDBSCAN identified\", len(np.unique(valid)), \"clusters.\")\nelif cluster_method == \"kmeans\":\n    n_samples = features_norm.shape[0]\n    K = min(max(2, n_samples // 3), n_samples)\n    if K < 1:\n        raise ValueError(\"Insufficient samples for k-means.\")\n    km = KMeans(n_clusters=K, random_state=42)\n    cluster_labels = km.fit_predict(features_norm)\n    print(\"k-means formed\", len(np.unique(cluster_labels)), \"clusters.\")\nelse:\n    raise ValueError(f\"Unsupported cluster_method: {cluster_method}\")\n\ngroup_cluster_map = {g: int(label) for g, label in zip(groups, cluster_labels)}\nsentences_df[\"topic\"] = sentences_df[\"group_id\"].map(group_cluster_map)\n\n# Compute centroids for each cluster\nclusters = np.unique(cluster_labels)\ncentroids = np.array(\n    [features_norm[cluster_labels == c].mean(axis=0) for c in clusters]\n)\n\n# Hierarchical clustering on centroids\nagg = AgglomerativeClustering(\n    n_clusters=None, distance_threshold=0.5, metric=\"euclidean\", linkage=\"average\",\n)\nhier_labels = agg.fit_predict(centroids)\nhier_map = {int(c): int(h) for c, h in zip(clusters, hier_labels)}\nsentences_df[\"super_topic\"] = sentences_df[\"topic\"].map(hier_map)\n\n# Persist group-level feature matrix for downstream analysis\ngroup_features_norm = features_norm\ngroup_ids_ordered = list(groups)\n\nsentences_df[[\"sentence_id\", \"group_id\", \"topic\", \"super_topic\"]].head()\n"
   "source": "## 5. Topic Labeling and Social Statistics\n\nTo make the discovered topics interpretable, we extract keywords and representative sentences. We employ **class‑based TF‑IDF (c‑TF‑IDF)**: for each topic, we concatenate all sentences in that cluster into a single document and compute TF‑IDF scores over the vocabulary. The top‑scoring n‑grams are selected as keywords.\n\nWe also present a few sentences closest to the cluster centroid in the fused embedding space.\n\nIn addition, we summarise the voting behaviour within each topic using the following statistics:\n\n* **Coverage:** the proportion of eligible participants who actually voted on sentences in this topic.\n* **Agree / Disagree / Pass:** the fraction of votes (among exposed participants) that were +1, -1 or 0, respectively.\n* **Polarity:** the mean vote value (agree = +1, disagree = -1, pass = 0).\n* **Controversy:** the entropy (base 3) of the agree/pass/disagree distribution; higher values indicate more mixed opinions.\n\nThese metrics help identify topics with strong consensus versus contentious topics.\n"
     "text": "Topic 0\n  Keywords: values, facct doing, doing, facct, values facct doing, values facct, community, open welcoming, open welcoming community, community facct building\n  Representative sentences:\n   - FAccT is building a community that shares similar values\n   - FAccT is doing well in sticking to their values\n   - FAccT is doing well in selecting diverse locations that are fun and culturally enriched.\n  Coverage: 88.28%, Agree: 92.0%, Disagree: 1.8%,\n        Pass: 6.2%, Polarity: 0.90, Controversy: 0.29\nTopic 1\n  Keywords: workshops, industry, facct, 24, ve facct, ve, panels, panels workshops, years, facct doing\n  Representative sentences:\n   - 3 times I've been to FAccT (18, 23, 24) tech industry influence (e.g. who runs workshops) has been noticeable & sometimes problematic.\n   - FAccT could require panels & workshops to be designed/led by representatives of that year's region. '24 had decent regional representation.\n   - In 3 years I've been to FAccT it had plenty of industry input through the papers chosen & who ran or spoke at panels & workshops.\n  Coverage: 85.94%, Agree: 67.3%, Disagree: 18.2%,\n        Pass: 14.5%, Polarity: 0.49, Controversy: 0.78\nTopic 2\n  Keywords: facct, big, volunteer, academics facct, think, minimum, volunteer run, different, run, academics\n  Representative sentences:\n   - FAccT should be free for phd students\n   - FAccT should NOT be sponsored by big corporates\n   - FAccT should remain volunteer-run (organizers are uncompensated)\n  Coverage: 91.41%, Agree: 81.2%, Disagree: 9.4%,\n        Pass: 9.4%, Polarity: 0.72, Controversy: 0.56\nTopic 3\n  Keywords: visa, facct, local, locations, attendees, run local, uk asia, uk, generator alternative solidarity, barriers\n  Representative sentences:\n   - FAccT should NOT be repeatedly in visa-requiring locations\n   - should take visa as a consideration. but \"should not\" sets a barriers for some locations benefiting for hosting FAccT\n   - Facct should also run local meetups (EU/UK/Asia-Pacific) in case if attendees fail to attend due to visa reasons\n  Coverage: 92.19%, Agree: 86.4%, Disagree: 2.5%,\n        Pass: 11.0%, Polarity: 0.84, Controversy: 0.42\nTopic 4\n  Keywords: faact, conference, future collaborators, useful space, useful space meet, cross, cross disciplinary, cross disciplinary meetings, cs, multidisciplinary conference computer\n  Representative sentences:\n   - Faact is mostly focused on a single discipline (CS)\n   - Faact is a useful space to meet future collaborators\n   - Faact should support more cross-disciplinary meetings at the conference, not just in the socials.\n  Coverage: 84.38%, Agree: 83.3%, Disagree: 2.8%,\n        Pass: 13.9%, Polarity: 0.81, Controversy: 0.48\nTopic 5\n  Keywords: room, facct, aware welcoming, better acoustics, aware welcoming neurodiversity, building low, better acoustics building, dismiss, dismiss environmental, dismiss environmental foot\n  Representative sentences:\n   - FAccT should NOT be exclusionary\n   - FAccT should NOT dismiss environmental foot print of research\n   - FAccT should diversify its epitstemology.\n  Coverage: 84.38%, Agree: 85.2%, Disagree: 2.8%,\n        Pass: 12.0%, Polarity: 0.82, Controversy: 0.45\nTopic 6\n  Keywords: conference, creating, design, participatory, participatory design, facct, facct doing, doing, building creating, building creating conference\n  Representative sentences:\n   - FAccT is not doing well in providing tangible, practical guidance for non-academics to build responsible technology\n   - FAccT is not doing well in creating opportunities for participatory design, prototyping, and building/creating together AT the conference\n   - FaCCT should engage in real participatory design\n  Coverage: 85.94%, Agree: 84.5%, Disagree: 5.5%,\n        Pass: 10.0%, Polarity: 0.79, Controversy: 0.48\nTopic 7\n  Keywords: narratives, western, attention, topics, facct, fields, relates topics receive, submission likely, perspective past, perspective\n  Representative sentences:\n   - FAccT should NOT be centred around western naratives (e.g. US right now) and political atmosphere\n   - agree on value of neurodiversity, yet, comtemplate \"more aware\" relates to what topics receive less attention,as attention isn't unlimited\n   - better question is the alternatives (how to) not center around western narratives\n  Coverage: 79.69%, Agree: 70.6%, Disagree: 10.8%,\n        Pass: 18.6%, Polarity: 0.60, Controversy: 0.73\nTopic 8\n  Keywords: sessions, poster sessions, poster, publications, esp, poster sessions facct, sessions facct, facct, audience esp, speed dating\n  Representative sentences:\n   - FAccT should have poster sessions\n   - the current feedback of poster sessions, esp junior\n   - Poster sessions provide value as an easy way to randomly talk to anyone and start a conversation, sometimes difficult otherwise at FAccT.\n  Coverage: 87.50%, Agree: 83.0%, Disagree: 8.0%,\n        Pass: 8.9%, Polarity: 0.75, Controversy: 0.52\nTopic 9\n  Keywords: facct, contribute plastic, waste, waste facct, waste facct multilingual, contribute plastic waste, plastic waste facct, plastic waste, plastic, facct vegetarian facct\n  Representative sentences:\n   - FAccT should be vegetarian\n   - FAccT should NOT contribute to plastic waste\n   - FAccT should be multilingual\n  Coverage: 85.94%, Agree: 82.7%, Disagree: 8.2%,\n        Pass: 9.1%, Polarity: 0.75, Controversy: 0.53\n"
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic</th>\n      <th>keywords</th>\n      <th>representatives</th>\n      <th>coverage</th>\n      <th>agree_pct</th>\n      <th>disagree_pct</th>\n      <th>pass_pct</th>\n      <th>polarity</th>\n      <th>controversy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[values, facct doing, doing, facct, values fac...</td>\n      <td>[FAccT is building a community that shares sim...</td>\n      <td>0.882812</td>\n      <td>0.920354</td>\n      <td>0.017699</td>\n      <td>0.061947</td>\n      <td>0.902655</td>\n      <td>0.291361</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[workshops, industry, facct, 24, ve facct, ve,...</td>\n      <td>[3 times I've been to FAccT (18, 23, 24) tech ...</td>\n      <td>0.859375</td>\n      <td>0.672727</td>\n      <td>0.181818</td>\n      <td>0.145455</td>\n      <td>0.490909</td>\n      <td>0.780124</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>[facct, big, volunteer, academics facct, think...</td>\n      <td>[FAccT should be free for phd students, FAccT ...</td>\n      <td>0.914062</td>\n      <td>0.811966</td>\n      <td>0.094017</td>\n      <td>0.094017</td>\n      <td>0.717949</td>\n      <td>0.558610</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>[visa, facct, local, locations, attendees, run...</td>\n      <td>[FAccT should NOT be repeatedly in visa-requir...</td>\n      <td>0.921875</td>\n      <td>0.864407</td>\n      <td>0.025424</td>\n      <td>0.110169</td>\n      <td>0.838983</td>\n      <td>0.420819</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>[faact, conference, future collaborators, usef...</td>\n      <td>[Faact is mostly focused on a single disciplin...</td>\n      <td>0.843750</td>\n      <td>0.833333</td>\n      <td>0.027778</td>\n      <td>0.138889</td>\n      <td>0.805556</td>\n      <td>0.478472</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>[room, facct, aware welcoming, better acoustic...</td>\n      <td>[FAccT should NOT be exclusionary, FAccT shoul...</td>\n      <td>0.843750</td>\n      <td>0.851852</td>\n      <td>0.027778</td>\n      <td>0.120370</td>\n      <td>0.824074</td>\n      <td>0.446906</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>[conference, creating, design, participatory, ...</td>\n      <td>[FAccT is not doing well in providing tangible...</td>\n      <td>0.859375</td>\n      <td>0.845455</td>\n      <td>0.054545</td>\n      <td>0.100000</td>\n      <td>0.790909</td>\n      <td>0.483202</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>[narratives, western, attention, topics, facct...</td>\n      <td>[FAccT should NOT be centred around western na...</td>\n      <td>0.796875</td>\n      <td>0.705882</td>\n      <td>0.107843</td>\n      <td>0.186275</td>\n      <td>0.598039</td>\n      <td>0.727353</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>[sessions, poster sessions, poster, publicatio...</td>\n      <td>[FAccT should have poster sessions, the curren...</td>\n      <td>0.875000</td>\n      <td>0.830357</td>\n      <td>0.080357</td>\n      <td>0.089286</td>\n      <td>0.750000</td>\n      <td>0.521268</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>[facct, contribute plastic, waste, waste facct...</td>\n      <td>[FAccT should be vegetarian, FAccT should NOT ...</td>\n      <td>0.859375</td>\n      <td>0.827273</td>\n      <td>0.081818</td>\n      <td>0.090909</td>\n      <td>0.745455</td>\n      <td>0.527639</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   topic                                           keywords  \\\n0      0  [values, facct doing, doing, facct, values fac...   \n1      1  [workshops, industry, facct, 24, ve facct, ve,...   \n2      2  [facct, big, volunteer, academics facct, think...   \n3      3  [visa, facct, local, locations, attendees, run...   \n4      4  [faact, conference, future collaborators, usef...   \n5      5  [room, facct, aware welcoming, better acoustic...   \n6      6  [conference, creating, design, participatory, ...   \n7      7  [narratives, western, attention, topics, facct...   \n8      8  [sessions, poster sessions, poster, publicatio...   \n9      9  [facct, contribute plastic, waste, waste facct...   \n\n                                     representatives  coverage  agree_pct  \\\n0  [FAccT is building a community that shares sim...  0.882812   0.920354   \n1  [3 times I've been to FAccT (18, 23, 24) tech ...  0.859375   0.672727   \n2  [FAccT should be free for phd students, FAccT ...  0.914062   0.811966   \n3  [FAccT should NOT be repeatedly in visa-requir...  0.921875   0.864407   \n4  [Faact is mostly focused on a single disciplin...  0.843750   0.833333   \n5  [FAccT should NOT be exclusionary, FAccT shoul...  0.843750   0.851852   \n6  [FAccT is not doing well in providing tangible...  0.859375   0.845455   \n7  [FAccT should NOT be centred around western na...  0.796875   0.705882   \n8  [FAccT should have poster sessions, the curren...  0.875000   0.830357   \n9  [FAccT should be vegetarian, FAccT should NOT ...  0.859375   0.827273   \n\n   disagree_pct  pass_pct  polarity  controversy  \n0      0.017699  0.061947  0.902655     0.291361  \n1      0.181818  0.145455  0.490909     0.780124  \n2      0.094017  0.094017  0.717949     0.558610  \n3      0.025424  0.110169  0.838983     0.420819  \n4      0.027778  0.138889  0.805556     0.478472  \n5      0.027778  0.120370  0.824074     0.446906  \n6      0.054545  0.100000  0.790909     0.483202  \n7      0.107843  0.186275  0.598039     0.727353  \n8      0.080357  0.089286  0.750000     0.521268  \n9      0.081818  0.090909  0.745455     0.527639  "
   "source": "import math\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\ndef compute_ctfidf(texts, labels, ngram_range=(1, 3), top_k=10):\n    unique_labels = np.unique(labels)\n    docs = [\" \".join(np.array(texts)[labels == label]) for label in unique_labels]\n    vectorizer = TfidfVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(\n        docs\n    )\n    tfidf_matrix = vectorizer.transform(docs)\n    feature_names = np.array(vectorizer.get_feature_names_out())\n    keywords = {}\n    for i, label in enumerate(unique_labels):\n        row = tfidf_matrix[i].toarray().flatten()\n        idx = row.argsort()[::-1][:top_k]\n        keywords[label] = feature_names[idx].tolist()\n    return keywords\n\n\n# Keywords per topic\nkeywords_per_topic = compute_ctfidf(\n    sentences_df[\"text\"].values,\n    sentences_df[\"topic\"].values,\n    ngram_range=(1, 3),\n    top_k=10,\n)\n\n# Representative sentences: nearest group centroids\nrep_sentences = {}\nif \"group_ids_ordered\" in globals():\n    group_ids_array = np.array(group_ids_ordered)\n    for c in clusters:\n        group_mask = np.where(cluster_labels == c)[0]\n        if group_mask.size == 0:\n            continue\n        centroid = centroids[np.where(clusters == c)[0][0]]\n        dists = np.linalg.norm(group_features_norm[group_mask] - centroid, axis=1)\n        nearest_group_indices = group_mask[np.argsort(dists)[:3]]\n        nearest_group_ids = group_ids_array[nearest_group_indices]\n        texts = (\n            sentences_df[sentences_df[\"group_id\"].isin(nearest_group_ids)]\n            .sort_values(\"timestamp\")[\"text\"]\n            .tolist()\n        )\n        rep_sentences[c] = texts[:3]\nelse:\n    rep_sentences = {c: [] for c in clusters}\n\n\ndef compute_topic_stats(topic_id):\n    idxs = sentences_df[sentences_df[\"topic\"] == topic_id].index\n    groups_in_topic = sentences_df.iloc[idxs][\"group_id\"].unique()\n    elig_users = 0\n    voted_users = 0\n    agree_cnt = 0\n    disagree_cnt = 0\n    pass_cnt = 0\n    for u in users:\n        eligible_any = False\n        votes_for_topic = []\n        for g in groups_in_topic:\n            if eligibility.loc[u, g]:\n                eligible_any = True\n                rows = votes_df[\n                    (votes_df[\"participant_id\"] == u) & (votes_df[\"group_id\"] == g)\n                ]\n                if not rows.empty:\n                    votes_for_topic.append(rows.iloc[0][\"vote\"])\n        if eligible_any:\n            elig_users += 1\n            if votes_for_topic:\n                voted_users += 1\n                # pick strongest vote for this topic\n                v = sorted(votes_for_topic, key=lambda x: (abs(x), x), reverse=True)[0]\n                if v == 1:\n                    agree_cnt += 1\n                elif v == -1:\n                    disagree_cnt += 1\n                else:\n                    pass_cnt += 1\n    coverage = voted_users / max(elig_users, 1)\n    total = agree_cnt + disagree_cnt + pass_cnt\n    if total > 0:\n        agree_pct = agree_cnt / total\n        disagree_pct = disagree_cnt / total\n        pass_pct = pass_cnt / total\n        polarity = (agree_cnt - disagree_cnt) / total\n        probs = np.array([agree_pct, pass_pct, disagree_pct])\n        entropy = -np.sum(probs * np.log(probs + 1e-12)) / np.log(3)\n    else:\n        agree_pct = disagree_pct = pass_pct = polarity = entropy = np.nan\n    return {\n        \"coverage\": coverage,\n        \"agree_pct\": agree_pct,\n        \"disagree_pct\": disagree_pct,\n        \"pass_pct\": pass_pct,\n        \"polarity\": polarity,\n        \"controversy\": entropy,\n    }\n\n\nsummary_rows = []\ntopic_stats = {c: compute_topic_stats(c) for c in clusters}\n\n# Display summary per topic\nfor c in clusters:\n    print(f\"Topic {c}\")\n    print(\"  Keywords:\", \", \".join(keywords_per_topic.get(c, [])))\n    print(\"  Representative sentences:\")\n    for s in rep_sentences.get(c, []):\n        print(\"   -\", s)\n    stats = topic_stats[c]\n    coverage_txt = f\"{stats['coverage']:.2%}\"\n    agree_txt = \"nan\" if np.isnan(stats[\"agree_pct\"]) else f\"{stats['agree_pct']:.1%}\"\n    disagree_txt = (\n        \"nan\" if np.isnan(stats[\"disagree_pct\"]) else f\"{stats['disagree_pct']:.1%}\"\n    )\n    pass_txt = \"nan\" if np.isnan(stats[\"pass_pct\"]) else f\"{stats['pass_pct']:.1%}\"\n    polarity_txt = \"nan\" if np.isnan(stats[\"polarity\"]) else f\"{stats['polarity']:.2f}\"\n    controversy_txt = (\n        \"nan\" if np.isnan(stats[\"controversy\"]) else f\"{stats['controversy']:.2f}\"\n    )\n    print(\n        f\"\"\"  Coverage: {coverage_txt}, Agree: {agree_txt}, Disagree: {disagree_txt},\n        Pass: {pass_txt}, Polarity: {polarity_txt}, Controversy: {controversy_txt}\"\"\"\n    )\n    summary_rows.append(\n        {\n            \"topic\": int(c),\n            \"keywords\": keywords_per_topic.get(c, []),\n            \"representatives\": rep_sentences.get(c, []),\n            \"coverage\": stats[\"coverage\"],\n            \"agree_pct\": stats[\"agree_pct\"],\n            \"disagree_pct\": stats[\"disagree_pct\"],\n            \"pass_pct\": stats[\"pass_pct\"],\n            \"polarity\": stats[\"polarity\"],\n            \"controversy\": stats[\"controversy\"],\n        }\n    )\n\nsummary_df = pd.DataFrame(summary_rows).sort_values(\"topic\").reset_index(drop=True)\nsummary_df"
   "source": "### Topic-level visualisations\n\nThese plots cross-reference topic size, vote mix, and derived metrics to highlight which themes draw consensus versus controversy."
     "text": "/tmp/ipykernel_5845/2135476098.py:15: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(\n/tmp/ipykernel_5845/2135476098.py:160: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(\n"
      "text/plain": "<Figure size 2160x1440 with 5 Axes>"
   "source": "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n\n# 1. Topic sizes\nif \"topic\" in sentences_df.columns:\n    topic_counts = (\n        sentences_df.groupby(\"topic\")\n        .agg(groups=(\"group_id\", \"nunique\"), sentences=(\"sentence_id\", \"nunique\"))\n        .reset_index()\n        .sort_values(\"groups\", ascending=False)\n    )\nelse:\n    topic_counts = pd.DataFrame()\n\nif not topic_counts.empty:\n    sns.barplot(\n        data=topic_counts,\n        x=\"groups\",\n        y=\"topic\",\n        palette=\"Blues_r\",\n        ax=axes[0, 0],\n    )\n    axes[0, 0].set_title(\"Sentence groups per topic\")\n    axes[0, 0].set_xlabel(\"Number of unique groups\")\n    axes[0, 0].set_ylabel(\"Topic\")\n    for container in axes[0, 0].containers:\n        axes[0, 0].bar_label(container, fmt=\"%.0f\")\nelse:\n    axes[0, 0].axis(\"off\")\n    axes[0, 0].text(\n        0.5, 0.5, \"No topic assignments available.\", ha=\"center\", va=\"center\"\n    )\n\n# 2. Vote sentiment mix\nif \"topic\" in sentences_df.columns:\n    topic_votes = votes_df.merge(\n        sentences_df[[\"group_id\", \"topic\"]], on=\"group_id\", how=\"left\"\n    )\nelse:\n    topic_votes = pd.DataFrame()\n\nif not topic_votes.empty:\n    vote_map = {1: \"Agree\", -1: \"Disagree\", 0: \"Pass\"}\n    topic_votes[\"vote_label\"] = topic_votes[\"vote\"].map(vote_map).fillna(\"Other\")\n    vote_share = (\n        topic_votes.groupby([\"topic\", \"vote_label\"]).size().unstack(fill_value=0)\n    )\n    ordered_cols = [\"Agree\", \"Pass\", \"Disagree\", \"Other\"]\n    vote_share = vote_share.reindex(columns=ordered_cols, fill_value=0)\n    vote_share_pct = vote_share.div(vote_share.sum(axis=1), axis=0).fillna(0) * 100\n\n    y_positions = np.arange(len(vote_share_pct.index))\n    start = np.zeros(len(y_positions))\n    colors = {\n        \"Agree\": \"#2ca02c\",\n        \"Pass\": \"#ffbb78\",\n        \"Disagree\": \"#d62728\",\n        \"Other\": \"#7f7f7f\",\n    }\n\n    for label in ordered_cols:\n        values = vote_share_pct[label].values\n        axes[0, 1].barh(\n            y_positions,\n            values,\n            left=start,\n            color=colors[label],\n            label=label,\n        )\n        for i, v in enumerate(values):\n            if v > 4:\n                axes[0, 1].text(\n                    start[i] + v / 2,\n                    y_positions[i],\n                    f\"{v:.0f}%\",\n                    ha=\"center\",\n                    va=\"center\",\n                    color=\"white\",\n                    fontsize=9,\n                )\n        start += values\n\n    axes[0, 1].set_yticks(y_positions)\n    axes[0, 1].set_yticklabels(vote_share_pct.index.astype(int))\n    axes[0, 1].invert_yaxis()\n    axes[0, 1].set_xlim(0, 100)\n    axes[0, 1].set_xlabel(\"Share of votes (%)\")\n    axes[0, 1].set_title(\"Vote sentiment mix by topic\")\n    axes[0, 1].legend(frameon=False, loc=\"lower right\")\nelse:\n    axes[0, 1].axis(\"off\")\n    axes[0, 1].text(\n        0.5, 0.5, \"No topic-level vote data available.\", ha=\"center\", va=\"center\"\n    )\n\n# Prepare summary merge\nif \"topic\" in summary_df.columns:\n    summary_plot = summary_df.copy()\nelse:\n    summary_plot = pd.DataFrame()\n\nif not summary_plot.empty:\n    summary_plot = summary_plot.merge(topic_counts, on=\"topic\", how=\"left\")\n\n# 3. Coverage vs controversy scatter\nif (\n    not summary_plot.empty\n    and summary_plot[[\"coverage\", \"controversy\"]].notna().any().any()\n):\n    scatter_df = summary_plot.dropna(subset=[\"coverage\", \"controversy\"]).copy()\n    if not scatter_df.empty:\n        scatter_df[\"coverage_pct\"] = scatter_df[\"coverage\"] * 100\n        scatter_sizes = np.clip(scatter_df[\"groups\"].fillna(1), 1, None) * 40\n        cmap = colormaps[\"coolwarm\"]\n        sc = axes[1, 0].scatter(\n            scatter_df[\"coverage_pct\"],\n            scatter_df[\"controversy\"],\n            s=scatter_sizes,\n            c=scatter_df[\"polarity\"],\n            cmap=cmap,\n            vmin=-1,\n            vmax=1,\n            alpha=0.85,\n            edgecolor=\"white\",\n            linewidth=0.5,\n        )\n        for _, row in scatter_df.iterrows():\n            axes[1, 0].text(\n                row[\"coverage_pct\"] + 0.5,\n                row[\"controversy\"] + 0.01,\n                int(row[\"topic\"]),\n                fontsize=9,\n            )\n        axes[1, 0].set_xlabel(\"Coverage (% of eligible participants voting)\")\n        axes[1, 0].set_ylabel(\"Controversy (normalised entropy)\")\n        axes[1, 0].set_title(\"Coverage vs. controversy by topic\")\n        cbar = plt.colorbar(sc, ax=axes[1, 0], shrink=0.8)\n        cbar.set_label(\"Polarity (agree ↔ disagree)\")\n    else:\n        axes[1, 0].axis(\"off\")\n        axes[1, 0].text(\n            0.5,\n            0.5,\n            \"Insufficient summary metrics for scatter plot.\",\n            ha=\"center\",\n            va=\"center\",\n        )\nelse:\n    axes[1, 0].axis(\"off\")\n    axes[1, 0].text(\n        0.5,\n        0.5,\n        \"Insufficient summary metrics for scatter plot.\",\n        ha=\"center\",\n        va=\"center\",\n    )\n\n# 4. Polarity bar chart\nif not summary_plot.empty and summary_plot[\"polarity\"].notna().any():\n    polarity_df = summary_plot.dropna(subset=[\"polarity\"]).sort_values(\"polarity\")\n    sns.barplot(\n        data=polarity_df,\n        y=\"polarity\",\n        x=\"topic\",\n        palette=\"coolwarm\",\n        ax=axes[1, 1],\n    )\n    # axes[1, 1].axhline(0.5, color=\"#555555\", linewidth=1)\n    axes[1, 1].set_title(\"Net agreement (polarity) by topic\")\n    axes[1, 1].set_xlabel(\"Polarity (agree minus disagree share)\")\n    axes[1, 1].set_ylabel(\"Topic\")\nelse:\n    axes[1, 1].axis(\"off\")\n    axes[1, 1].text(0.5, 0.5, \"Polarity scores unavailable.\", ha=\"center\", va=\"center\")\n\nplt.suptitle(\"Topic-level summaries\", y=1.02)\nplt.tight_layout()\nplt.show()"
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic</th>\n      <th>keywords</th>\n      <th>representatives</th>\n      <th>coverage</th>\n      <th>agree_pct</th>\n      <th>disagree_pct</th>\n      <th>pass_pct</th>\n      <th>polarity</th>\n      <th>controversy</th>\n      <th>groups</th>\n      <th>sentences</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[workshops, industry, facct, 24, ve facct, ve,...</td>\n      <td>[3 times I've been to FAccT (18, 23, 24) tech ...</td>\n      <td>0.859375</td>\n      <td>0.672727</td>\n      <td>0.181818</td>\n      <td>0.145455</td>\n      <td>0.490909</td>\n      <td>0.780124</td>\n      <td>6</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>[narratives, western, attention, topics, facct...</td>\n      <td>[FAccT should NOT be centred around western na...</td>\n      <td>0.796875</td>\n      <td>0.705882</td>\n      <td>0.107843</td>\n      <td>0.186275</td>\n      <td>0.598039</td>\n      <td>0.727353</td>\n      <td>7</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>[facct, big, volunteer, academics facct, think...</td>\n      <td>[FAccT should be free for phd students, FAccT ...</td>\n      <td>0.914062</td>\n      <td>0.811966</td>\n      <td>0.094017</td>\n      <td>0.094017</td>\n      <td>0.717949</td>\n      <td>0.558610</td>\n      <td>10</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>[facct, contribute plastic, waste, waste facct...</td>\n      <td>[FAccT should be vegetarian, FAccT should NOT ...</td>\n      <td>0.859375</td>\n      <td>0.827273</td>\n      <td>0.081818</td>\n      <td>0.090909</td>\n      <td>0.745455</td>\n      <td>0.527639</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>[sessions, poster sessions, poster, publicatio...</td>\n      <td>[FAccT should have poster sessions, the curren...</td>\n      <td>0.875000</td>\n      <td>0.830357</td>\n      <td>0.080357</td>\n      <td>0.089286</td>\n      <td>0.750000</td>\n      <td>0.521268</td>\n      <td>6</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>[conference, creating, design, participatory, ...</td>\n      <td>[FAccT is not doing well in providing tangible...</td>\n      <td>0.859375</td>\n      <td>0.845455</td>\n      <td>0.054545</td>\n      <td>0.100000</td>\n      <td>0.790909</td>\n      <td>0.483202</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>[faact, conference, future collaborators, usef...</td>\n      <td>[Faact is mostly focused on a single disciplin...</td>\n      <td>0.843750</td>\n      <td>0.833333</td>\n      <td>0.027778</td>\n      <td>0.138889</td>\n      <td>0.805556</td>\n      <td>0.478472</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>[room, facct, aware welcoming, better acoustic...</td>\n      <td>[FAccT should NOT be exclusionary, FAccT shoul...</td>\n      <td>0.843750</td>\n      <td>0.851852</td>\n      <td>0.027778</td>\n      <td>0.120370</td>\n      <td>0.824074</td>\n      <td>0.446906</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>[visa, facct, local, locations, attendees, run...</td>\n      <td>[FAccT should NOT be repeatedly in visa-requir...</td>\n      <td>0.921875</td>\n      <td>0.864407</td>\n      <td>0.025424</td>\n      <td>0.110169</td>\n      <td>0.838983</td>\n      <td>0.420819</td>\n      <td>7</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[values, facct doing, doing, facct, values fac...</td>\n      <td>[FAccT is building a community that shares sim...</td>\n      <td>0.882812</td>\n      <td>0.920354</td>\n      <td>0.017699</td>\n      <td>0.061947</td>\n      <td>0.902655</td>\n      <td>0.291361</td>\n      <td>7</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   topic                                           keywords  \\\n1      1  [workshops, industry, facct, 24, ve facct, ve,...   \n7      7  [narratives, western, attention, topics, facct...   \n2      2  [facct, big, volunteer, academics facct, think...   \n9      9  [facct, contribute plastic, waste, waste facct...   \n8      8  [sessions, poster sessions, poster, publicatio...   \n6      6  [conference, creating, design, participatory, ...   \n4      4  [faact, conference, future collaborators, usef...   \n5      5  [room, facct, aware welcoming, better acoustic...   \n3      3  [visa, facct, local, locations, attendees, run...   \n0      0  [values, facct doing, doing, facct, values fac...   \n\n                                     representatives  coverage  agree_pct  \\\n1  [3 times I've been to FAccT (18, 23, 24) tech ...  0.859375   0.672727   \n7  [FAccT should NOT be centred around western na...  0.796875   0.705882   \n2  [FAccT should be free for phd students, FAccT ...  0.914062   0.811966   \n9  [FAccT should be vegetarian, FAccT should NOT ...  0.859375   0.827273   \n8  [FAccT should have poster sessions, the curren...  0.875000   0.830357   \n6  [FAccT is not doing well in providing tangible...  0.859375   0.845455   \n4  [Faact is mostly focused on a single disciplin...  0.843750   0.833333   \n5  [FAccT should NOT be exclusionary, FAccT shoul...  0.843750   0.851852   \n3  [FAccT should NOT be repeatedly in visa-requir...  0.921875   0.864407   \n0  [FAccT is building a community that shares sim...  0.882812   0.920354   \n\n   disagree_pct  pass_pct  polarity  controversy  groups  sentences  \n1      0.181818  0.145455  0.490909     0.780124       6          6  \n7      0.107843  0.186275  0.598039     0.727353       7          7  \n2      0.094017  0.094017  0.717949     0.558610      10         11  \n9      0.081818  0.090909  0.745455     0.527639       3          3  \n8      0.080357  0.089286  0.750000     0.521268       6          6  \n6      0.054545  0.100000  0.790909     0.483202       5          5  \n4      0.027778  0.138889  0.805556     0.478472       4          4  \n5      0.027778  0.120370  0.824074     0.446906       5          5  \n3      0.025424  0.110169  0.838983     0.420819       7          7  \n0      0.017699  0.061947  0.902655     0.291361       7          7  "
   "source": "polarity_df"
   "source": "## Conclusion\n\nThis notebook demonstrated an end‑to‑end approach to analysing single‑sentence submissions with crowd‑sourced votes. We handled semantic redundancy via paraphrase mining, estimated exposure probabilities to correct for non‑uniform visibility, learned vote‑driven embeddings through signed matrix factorisation weighted by inverse propensities, fused them with semantic sentence embeddings, clustered the fused vectors to find topics, and labelled those topics with keywords and social statistics.\n\nThe techniques showcased here are modular: you can swap out the embedding models, use alternative exposure models or matrix factorisation algorithms, or experiment with different clustering methods. The general principle remains: **model exposure**, **learn meaningful representations**, **cluster to discover structure**, and **provide interpretable summaries**.\n"
   "source": ""
