{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccaf7d18",
   "metadata": {},
   "source": [
    "# Discovering Topics from Voted Sentences\n",
    "\n",
    "This notebook walks through a complete pipeline for extracting **topics** from a collection of single-sentence submissions that have been rated by participants. Each sentence can be voted on with one of three outcomes: **agree** (1), **disagree** (-1), or **pass/no opinion** (0). Participants joined at different times and new sentences were added over time, so some sentences were never exposed to earlier participants.  \n",
    "\n",
    "The pipeline addresses several challenges:\n",
    "\n",
    "1. **Semantic deduplication:** multiple sentences may express the same idea; we group paraphrases into a single cluster before analysing votes.\n",
    "2. **Exposure modelling:** because participants see only a subset of sentences (and exposure is not uniform), we model the probability that a participant has seen a sentence and correct for this in the vote-based embeddings.\n",
    "3. **Signed matrix factorization with propensity weighting:** we learn low-dimensional representations of sentences based on participants' votes, using inverse-propensity weights to remove exposure bias and a special treatment for pass votes.\n",
    "4. **Feature fusion and clustering:** we combine vote-derived embeddings with semantic sentence embeddings and cluster them using a density-based algorithm. An agglomerative step builds a hierarchy of topics.\n",
    "5. **Topic labeling and social statistics:** we label clusters with keywords (via c‑TF‑IDF), show representative sentences, and summarise each topic with the distribution of agree/disagree/pass votes.\n",
    "\n",
    "Throughout the notebook we provide commentary to explain each step and choices made. Feel free to adjust hyperparameters, thresholds and weighting schemes to your data and domain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2f4ad9",
   "metadata": {},
   "source": [
    "## Installation (optional)\n",
    "\n",
    "The environment used to run this notebook may already have the necessary libraries installed. If you encounter import errors, uncomment the following lines and run them to install the required packages:\n",
    "\n",
    "```python\n",
    "# !pip install sentence-transformers hdbscan umap-learn faiss-cpu networkx scikit-learn\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5789ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Attempt to read the data from CSV. If unavailable, create a synthetic dataset for demonstration.\n",
    "try:\n",
    "    sentences_df = pd.read_csv(\"sentences.csv\")\n",
    "    votes_df = pd.read_csv(\"votes.csv\")\n",
    "    sentences_df[\"timestamp\"] = pd.to_datetime(sentences_df[\"timestamp\"])\n",
    "    votes_df[\"timestamp\"] = pd.to_datetime(votes_df[\"timestamp\"])\n",
    "    print(\"Loaded sentences.csv and votes.csv.\")\n",
    "except Exception as e:\n",
    "    print(\"Could not load data files; generating a synthetic dataset instead:\", e)\n",
    "    # Synthetic dataset parameters\n",
    "    num_sentences = 128\n",
    "    num_participants = 60\n",
    "    np.random.seed(0)\n",
    "    # Create synthetic sentences with timestamps spread over 4 months\n",
    "    base_time = np.datetime64(\"2025-01-01\")\n",
    "    times = base_time + np.random.randint(0, 120, size=num_sentences).astype(\n",
    "        \"timedelta64[D]\"\n",
    "    )\n",
    "    sentences_df = pd.DataFrame(\n",
    "        {\n",
    "            \"sentence_id\": np.arange(num_sentences),\n",
    "            \"text\": [f\"Synthetic sentence {i}\" for i in range(num_sentences)],\n",
    "            \"timestamp\": times,\n",
    "        }\n",
    "    )\n",
    "    # Generate participant join times randomly over the same period\n",
    "    join_times = base_time + np.random.randint(0, 120, size=num_participants).astype(\n",
    "        \"timedelta64[D]\"\n",
    "    )\n",
    "    # Synthetic votes: each participant votes on sentences added before they joined, with random agree/disagree/pass\n",
    "    votes_records = []\n",
    "    for p, join_time in enumerate(join_times):\n",
    "        eligible_sentences = sentences_df[sentences_df[\"timestamp\"] <= join_time]\n",
    "        chosen = eligible_sentences.sample(frac=0.5, random_state=p)\n",
    "        for _, row in chosen.iterrows():\n",
    "            vote = np.random.choice([1, -1, 0], p=[0.5, 0.3, 0.2])\n",
    "            votes_records.append(\n",
    "                {\n",
    "                    \"participant_id\": p,\n",
    "                    \"sentence_id\": row[\"sentence_id\"],\n",
    "                    \"vote\": vote,\n",
    "                    \"timestamp\": join_time,\n",
    "                }\n",
    "            )\n",
    "    votes_df = pd.DataFrame(votes_records)\n",
    "    votes_df[\"timestamp\"] = pd.to_datetime(votes_df[\"timestamp\"])\n",
    "    print(\"Synthetic dataset created.\")\n",
    "\n",
    "sentences_df.head(), votes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e103ac2",
   "metadata": {},
   "source": [
    "## 1. Semantic Deduplication\n",
    "\n",
    "Multiple participants may express the same idea using different words. Before aggregating votes, we group **paraphrases** into single units called *groups*. Our deduplication strategy proceeds in two stages:\n",
    "\n",
    "1. **Bi‑encoder retrieval:** We embed each sentence with a sentence‑level transformer (e.g. `all‑mpnet‑base‑v2`) and use cosine similarity to find candidate paraphrase pairs. Embedding similarity is fast and yields high recall but may include false positives.\n",
    "2. **Cross‑encoder re‑ranking:** We pass each candidate pair through a more accurate cross‑encoder model (e.g. `ms‑marco‑MiniLM‑L‑6‑v2`). This network jointly processes both sentences and predicts a similarity score. We keep pairs above a chosen threshold as paraphrases.\n",
    "\n",
    "The union of verified paraphrase pairs forms a graph; each connected component corresponds to one semantic group.  \n",
    "\n",
    "If the `sentence-transformers` or `networkx` packages are unavailable, this step gracefully degrades by skipping deduplication (each sentence becomes its own group).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8a618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to import sentence-transformers. If unavailable, deduplication will be skipped.\n",
    "try:\n",
    "    from sentence_transformers import CrossEncoder, SentenceTransformer, util\n",
    "except ImportError as e:\n",
    "    print(\"sentence-transformers not available:\", e)\n",
    "    SentenceTransformer = None\n",
    "    util = None\n",
    "    CrossEncoder = None\n",
    "\n",
    "# Compute bi‑encoder embeddings if possible\n",
    "bi_encoder = None\n",
    "sent_emb = None\n",
    "if SentenceTransformer is not None:\n",
    "    try:\n",
    "        bi_encoder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "        sent_emb = bi_encoder.encode(\n",
    "            sentences_df[\"text\"].tolist(),\n",
    "            convert_to_tensor=True,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Bi‑encoder unavailable or failed:\", e)\n",
    "        bi_encoder = None\n",
    "        sent_emb = None\n",
    "\n",
    "# Identify candidate paraphrase pairs using embedding similarity\n",
    "paraphrase_pairs = []\n",
    "if sent_emb is not None and util is not None:\n",
    "    candidates = util.paraphrase_mining_embeddings(sent_emb, top_k=20)\n",
    "    COS_THRESHOLD = 0.80\n",
    "    paraphrase_pairs = [(i, j) for score, i, j in candidates if score >= COS_THRESHOLD]\n",
    "\n",
    "# Refine pairs using cross‑encoder\n",
    "verified_pairs = []\n",
    "if paraphrase_pairs:\n",
    "    if CrossEncoder is not None:\n",
    "        try:\n",
    "            ce_model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "            texts = sentences_df[\"text\"].tolist()\n",
    "            pair_texts = [(texts[i], texts[j]) for i, j in paraphrase_pairs]\n",
    "            ce_scores = ce_model.predict(pair_texts)\n",
    "            CE_THRESHOLD = 0.50\n",
    "            verified_pairs = [\n",
    "                (i, j)\n",
    "                for (i, j), s in zip(paraphrase_pairs, ce_scores)\n",
    "                if s >= CE_THRESHOLD\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            print(\"Cross‑encoder unavailable or failed:\", e)\n",
    "            verified_pairs = paraphrase_pairs.copy()\n",
    "    else:\n",
    "        verified_pairs = paraphrase_pairs.copy()\n",
    "\n",
    "# Build connected components of paraphrase graph\n",
    "try:\n",
    "    import networkx as nx\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(len(sentences_df)))\n",
    "    G.add_edges_from(verified_pairs)\n",
    "    components = list(nx.connected_components(G))\n",
    "except Exception as e:\n",
    "    print(\"networkx unavailable or dedup skipped:\", e)\n",
    "    components = [{i} for i in range(len(sentences_df))]\n",
    "\n",
    "# Assign a group_id per sentence\n",
    "group_mapping = {}\n",
    "for cid, comp in enumerate(components):\n",
    "    for idx in comp:\n",
    "        sid = sentences_df.iloc[idx][\"sentence_id\"]\n",
    "        group_mapping[sid] = cid\n",
    "sentences_df[\"group_id\"] = sentences_df[\"sentence_id\"].map(group_mapping)\n",
    "\n",
    "# Merge group_id into votes_df\n",
    "votes_df = votes_df.merge(\n",
    "    sentences_df[[\"sentence_id\", \"group_id\"]], on=\"sentence_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "print(f\"Deduplication complete: {len(components)} groups identified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7417a1aa",
   "metadata": {},
   "source": [
    "## 2. Eligibility and Exposure Propensity\n",
    "\n",
    "Participants joined at various times; sentences were added over time. A participant *cannot* vote on a sentence that was submitted after they left the study. To correct for this **structural missingness**, we build an *eligibility matrix* `E` where `E[i, g] = 1` if participant `i` could have seen sentence group `g`, and `0` otherwise.  \n",
    "\n",
    "Within the eligible pairs there is still **selection bias**: some sentences may be more likely to be shown or noticed than others. We model the probability that an eligible participant votes on a sentence using logistic regression.  \n",
    "\n",
    "Features used in the exposure model:\n",
    "\n",
    "* `sentence_age`: time difference (in days) between the participant’s last vote and the sentence’s submission.\n",
    "* `active_days`: the span (in days) of each participant’s activity (last vote minus first vote).\n",
    "\n",
    "The binary target is 1 if the participant voted on the sentence, 0 otherwise. The resulting predicted probability `p_hat` serves as our **propensity score**. If `scikit-learn` is unavailable, we fall back to a simple ratio: for each group, `p_hat = (# voters)/(# eligible)`.  \n",
    "\n",
    "We will later weight each observed vote by `1 / p_hat` to account for non-uniform exposure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b3f387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Compute last and first vote times per participant\n",
    "last_vote_time = votes_df.groupby(\"participant_id\")[\"timestamp\"].max()\n",
    "first_vote_time = votes_df.groupby(\"participant_id\")[\"timestamp\"].min()\n",
    "participant_active_days = (last_vote_time - first_vote_time).dt.total_seconds() / (\n",
    "    24 * 3600.0\n",
    ")\n",
    "\n",
    "# Eligibility matrix: participants x group_id\n",
    "users = votes_df[\"participant_id\"].unique()\n",
    "groups = sentences_df[\"group_id\"].unique()\n",
    "eligibility = pd.DataFrame(False, index=users, columns=groups)\n",
    "\n",
    "# Map group to sentence timestamp\n",
    "group_time_map = sentences_df.set_index(\"group_id\")[\"timestamp\"].to_dict()\n",
    "\n",
    "# Populate eligibility: participant can see group if their last vote is after sentence timestamp\n",
    "for u in users:\n",
    "    last_time = last_vote_time[u]\n",
    "    for g in groups:\n",
    "        if last_time >= group_time_map[g]:\n",
    "            eligibility.loc[u, g] = True\n",
    "\n",
    "# Build data for logistic regression: features and labels for eligible pairs\n",
    "rows = []\n",
    "for u in users:\n",
    "    for g in groups:\n",
    "        if not eligibility.loc[u, g]:\n",
    "            continue\n",
    "        sent_time = group_time_map[g]\n",
    "        lv_time = last_vote_time[u]\n",
    "        sentence_age = (lv_time - sent_time).total_seconds() / (24 * 3600.0)\n",
    "        active_days = participant_active_days[u]\n",
    "        # outcome: 1 if user voted on this group\n",
    "        voted = int(\n",
    "            ((votes_df[\"participant_id\"] == u) & (votes_df[\"group_id\"] == g)).any()\n",
    "        )\n",
    "        rows.append(\n",
    "            {\n",
    "                \"participant_id\": u,\n",
    "                \"group_id\": g,\n",
    "                \"sentence_age\": sentence_age,\n",
    "                \"active_days\": active_days,\n",
    "                \"voted\": voted,\n",
    "            }\n",
    "        )\n",
    "\n",
    "exposure_df = pd.DataFrame(rows)\n",
    "\n",
    "# Normalize features\n",
    "X = exposure_df[[\"sentence_age\", \"active_days\"]].fillna(0.0).values\n",
    "y = exposure_df[\"voted\"].values\n",
    "mean = X.mean(axis=0)\n",
    "std = X.std(axis=0) + 1e-12\n",
    "X_norm = (X - mean) / std\n",
    "\n",
    "# Initialise p_hat DataFrame\n",
    "p_hat = pd.DataFrame(index=users, columns=groups, data=np.nan)\n",
    "\n",
    "try:\n",
    "    # Fit logistic regression\n",
    "    clf = LogisticRegression(max_iter=200)\n",
    "    clf.fit(X_norm, y)\n",
    "    preds = clf.predict_proba(X_norm)[:, 1]\n",
    "    exposure_df[\"probability\"] = preds\n",
    "    # Fill p_hat for eligible pairs\n",
    "    for u, g, p in zip(\n",
    "        exposure_df[\"participant_id\"],\n",
    "        exposure_df[\"group_id\"],\n",
    "        exposure_df[\"probability\"],\n",
    "    ):\n",
    "        p_hat.loc[u, g] = p\n",
    "    print(\"Exposure model fit with logistic regression.\")\n",
    "except Exception as e:\n",
    "    # Fallback: ratio (#voters)/(#eligible) per group\n",
    "    print(\"Logistic regression unavailable, using exposure ratio. Error:\", e)\n",
    "    n_voters = votes_df.groupby(\"group_id\")[\"participant_id\"].nunique()\n",
    "    n_eligible = eligibility.sum(axis=0)\n",
    "    for g in groups:\n",
    "        prob = n_voters.get(g, 0) / max(n_eligible[g], 1)\n",
    "        for u in users:\n",
    "            if eligibility.loc[u, g]:\n",
    "                p_hat.loc[u, g] = prob\n",
    "\n",
    "# Display head of eligibility and propensity matrices\n",
    "display(eligibility.head(), p_hat.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3926e02",
   "metadata": {},
   "source": [
    "## 3. Vote Embedding via Signed Matrix Factorization with Propensity Weighting\n",
    "\n",
    "Our next goal is to represent each sentence group by a low‑dimensional vector capturing how participants voted on it. We use a **signed matrix factorization** model with latent user and group factors (\\(U\\) and \\(V\\)) and biases (\\(b_u\\) and \\(c_v\\)). For each observed vote \\(y_{ij}\\) on group \\(j\\) by user \\(i\\), the score is\n",
    "\n",
    "\\[s_{ij} = U_i \\cdot V_j + b_{u_i} + c_{v_j}\\].\n",
    "\n",
    "We minimise a weighted loss:\n",
    "\n",
    "* **Agree/Disagree (±1):** logistic loss \\(\\log(1+\\exp(-y\\, s))\\).\n",
    "* **Pass (0):** quadratic loss \\(w_0 \\cdot s^2\\) pulling the score toward neutrality, with hyperparameter \\(w_0\\).\n",
    "\n",
    "Each term is weighted by the **inverse propensity** \\(1/p_{ij}\\) computed earlier to correct for selection bias. We also include \\(\\ell_2\\) regularisation on the latent factors. Optimisation is performed with stochastic gradient descent (SGD).  \n",
    "\n",
    "The resulting item factors \\(V\\) serve as our vote‑based embeddings for each sentence group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06522b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "k = 16  # latent dimensionality\n",
    "lambda_reg = 1e-4  # regularisation strength\n",
    "w0 = 0.3  # weight for neutral votes\n",
    "epochs = 200\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Map users and groups to indices\n",
    "user_to_idx = {u: idx for idx, u in enumerate(users)}\n",
    "group_to_idx = {g: idx for idx, g in enumerate(groups)}\n",
    "\n",
    "# Build list of (u_idx, g_idx, y, p_hat)\n",
    "train_data = []\n",
    "for _, row in votes_df.iterrows():\n",
    "    u_idx = user_to_idx[row[\"participant_id\"]]\n",
    "    g_idx = group_to_idx[row[\"group_id\"]]\n",
    "    y_val = row[\"vote\"]\n",
    "    p_val = p_hat.loc[row[\"participant_id\"], row[\"group_id\"]]\n",
    "    if p_val and not np.isnan(p_val) and p_val > 0:\n",
    "        train_data.append((u_idx, g_idx, y_val, p_val))\n",
    "\n",
    "# Initialise latent factors and biases\n",
    "num_users = len(users)\n",
    "num_items = len(groups)\n",
    "rng = np.random.default_rng(seed=42)\n",
    "U = 0.1 * rng.standard_normal((num_users, k))\n",
    "V = 0.1 * rng.standard_normal((num_items, k))\n",
    "b_u = np.zeros(num_users)\n",
    "c_v = np.zeros(num_items)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    np.random.shuffle(train_data)\n",
    "    total_loss = 0.0\n",
    "    for u_idx, g_idx, y_val, p_val in train_data:\n",
    "        s = np.dot(U[u_idx], V[g_idx]) + b_u[u_idx] + c_v[g_idx]\n",
    "        if y_val == 1 or y_val == -1:\n",
    "            loss = np.log(1 + np.exp(-y_val * s))\n",
    "            grad_s = -y_val * sigmoid(-y_val * s)\n",
    "        else:\n",
    "            loss = w0 * (s**2)\n",
    "            grad_s = 2 * w0 * s\n",
    "        # Inverse propensity weight\n",
    "        loss *= 1.0 / p_val\n",
    "        grad_s *= 1.0 / p_val\n",
    "        total_loss += loss\n",
    "        # Compute gradients with regularisation\n",
    "        grad_u = grad_s * V[g_idx] + 2 * lambda_reg * U[u_idx]\n",
    "        grad_v = grad_s * U[u_idx] + 2 * lambda_reg * V[g_idx]\n",
    "        # Update parameters\n",
    "        U[u_idx] -= learning_rate * grad_u\n",
    "        V[g_idx] -= learning_rate * grad_v\n",
    "        b_u[u_idx] -= learning_rate * grad_s\n",
    "        c_v[g_idx] -= learning_rate * grad_s\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, loss = {total_loss:.4f}\")\n",
    "\n",
    "vote_embeddings = V.copy()\n",
    "print(\"First 5 vote embeddings:\", vote_embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db75dc0e",
   "metadata": {},
   "source": [
    "## 4. Text Embedding and Feature Fusion\n",
    "\n",
    "While vote embeddings capture participants' opinions, we also want to incorporate **semantic information** from the sentences themselves. We obtain sentence embeddings using the same bi‑encoder as in the deduplication step (if available).  \n",
    "\n",
    "We standardise (zero‑mean/ unit‑variance) both vote and text embeddings and concatenate them. A normalisation to unit length ensures that cosine distance is equivalent to Euclidean distance on the sphere. We then cluster the fused representations using **HDBSCAN**, which automatically determines the number of clusters and handles noise. If `hdbscan` is unavailable, we fall back to `k`‑means.\n",
    "\n",
    "After clustering, we compute centroids of each topic and apply agglomerative clustering to obtain a hierarchy of topics (super‑topics). The labels `topic` and `super_topic` are added to the `sentences_df` DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e9d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "\n",
    "## Text embedding\n",
    "text_embeddings = None\n",
    "if \"bi_encoder\" in globals() and bi_encoder is not None:\n",
    "    try:\n",
    "        text_embeddings = bi_encoder.encode(\n",
    "            sentences_df[\"text\"].tolist(),\n",
    "            convert_to_tensor=False,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Error computing text embeddings:\", e)\n",
    "\n",
    "## Standardise vote and text embeddings\n",
    "vote_scaled = None\n",
    "if \"vote_embeddings\" in globals() and vote_embeddings is not None:\n",
    "    vote_scaler = StandardScaler()\n",
    "    vote_scaled = vote_scaler.fit_transform(vote_embeddings)\n",
    "\n",
    "text_scaled = None\n",
    "if text_embeddings is not None:\n",
    "    text_scaler = StandardScaler()\n",
    "    text_scaled = text_scaler.fit_transform(text_embeddings)\n",
    "\n",
    "# Build feature matrix\n",
    "if vote_scaled is not None and text_scaled is not None:\n",
    "    features = np.hstack([vote_scaled, text_scaled])\n",
    "elif vote_scaled is not None:\n",
    "    features = vote_scaled\n",
    "elif text_scaled is not None:\n",
    "    features = text_scaled\n",
    "else:\n",
    "    raise ValueError(\"No features available for clustering.\")\n",
    "\n",
    "# Normalise to unit norm\n",
    "features_norm = normalize(features)\n",
    "\n",
    "## Clustering\n",
    "try:\n",
    "    import hdbscan\n",
    "\n",
    "    hdb = hdbscan.HDBSCAN(min_cluster_size=15, min_samples=5, metric=\"euclidean\")\n",
    "    cluster_labels = hdb.fit_predict(features_norm)\n",
    "    print(\n",
    "        \"HDBSCAN identified\",\n",
    "        len(np.unique(cluster_labels[cluster_labels >= 0])),\n",
    "        \"clusters.\",\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"HDBSCAN unavailable, falling back to k‑means. Error:\", e)\n",
    "    K = 10\n",
    "    km = KMeans(n_clusters=K, random_state=42)\n",
    "    cluster_labels = km.fit_predict(features_norm)\n",
    "    print(\"k‑means formed\", len(np.unique(cluster_labels)), \"clusters.\")\n",
    "\n",
    "sentences_df[\"topic\"] = cluster_labels\n",
    "\n",
    "# Compute centroids for each cluster\n",
    "clusters = np.unique(cluster_labels)\n",
    "centroids = np.array(\n",
    "    [features_norm[cluster_labels == c].mean(axis=0) for c in clusters]\n",
    ")\n",
    "\n",
    "# Hierarchical clustering on centroids\n",
    "agg = AgglomerativeClustering(\n",
    "    n_clusters=None, distance_threshold=0.5, affinity=\"euclidean\", linkage=\"average\"\n",
    ")\n",
    "hier_labels = agg.fit_predict(centroids)\n",
    "hier_map = {c: h for c, h in zip(clusters, hier_labels)}\n",
    "sentences_df[\"super_topic\"] = sentences_df[\"topic\"].map(hier_map)\n",
    "\n",
    "sentences_df[[\"sentence_id\", \"group_id\", \"topic\", \"super_topic\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08b0448",
   "metadata": {},
   "source": [
    "## 5. Topic Labeling and Social Statistics\n",
    "\n",
    "To make the discovered topics interpretable, we extract keywords and representative sentences. We employ **class‑based TF‑IDF (c‑TF‑IDF)**: for each topic, we concatenate all sentences in that cluster into a single document and compute TF‑IDF scores over the vocabulary. The top‑scoring n‑grams are selected as keywords.\n",
    "\n",
    "We also present a few sentences closest to the cluster centroid in the fused embedding space.\n",
    "\n",
    "In addition, we summarise the voting behaviour within each topic using the following statistics:\n",
    "\n",
    "* **Coverage:** the proportion of eligible participants who actually voted on sentences in this topic.\n",
    "* **Agree / Disagree / Pass:** the fraction of votes (among exposed participants) that were +1, -1 or 0, respectively.\n",
    "* **Polarity:** the mean vote value (agree = +1, disagree = -1, pass = 0).\n",
    "* **Controversy:** the entropy (base 3) of the agree/pass/disagree distribution; higher values indicate more mixed opinions.\n",
    "\n",
    "These metrics help identify topics with strong consensus versus contentious topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac4ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def compute_ctfidf(texts, labels, ngram_range=(1, 3), top_k=10):\n",
    "    unique_labels = np.unique(labels)\n",
    "    docs = [\" \".join(np.array(texts)[labels == label]) for label in unique_labels]\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(\n",
    "        docs\n",
    "    )\n",
    "    tfidf_matrix = vectorizer.transform(docs)\n",
    "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "    keywords = {}\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        row = tfidf_matrix[i].toarray().flatten()\n",
    "        idx = row.argsort()[::-1][:top_k]\n",
    "        keywords[label] = feature_names[idx].tolist()\n",
    "    return keywords\n",
    "\n",
    "\n",
    "# Keywords per topic\n",
    "keywords_per_topic = compute_ctfidf(\n",
    "    sentences_df[\"text\"].values,\n",
    "    sentences_df[\"topic\"].values,\n",
    "    ngram_range=(1, 3),\n",
    "    top_k=10,\n",
    ")\n",
    "\n",
    "# Representative sentences: nearest to centroid\n",
    "rep_sentences = {}\n",
    "for c in clusters:\n",
    "    idxs = np.where(sentences_df[\"topic\"].values == c)[0]\n",
    "    if len(idxs) == 0:\n",
    "        continue\n",
    "    centroid = centroids[np.where(clusters == c)[0][0]]\n",
    "    dists = np.linalg.norm(features_norm[idxs] - centroid, axis=1)\n",
    "    nearest = idxs[np.argsort(dists)[:3]]\n",
    "    rep_sentences[c] = sentences_df.iloc[nearest][\"text\"].tolist()\n",
    "\n",
    "\n",
    "def compute_topic_stats(topic_id):\n",
    "    idxs = sentences_df[sentences_df[\"topic\"] == topic_id].index\n",
    "    groups_in_topic = sentences_df.iloc[idxs][\"group_id\"].unique()\n",
    "    elig_users = 0\n",
    "    voted_users = 0\n",
    "    agree_cnt = 0\n",
    "    disagree_cnt = 0\n",
    "    pass_cnt = 0\n",
    "    for u in users:\n",
    "        eligible_any = False\n",
    "        votes_for_topic = []\n",
    "        for g in groups_in_topic:\n",
    "            if eligibility.loc[u, g]:\n",
    "                eligible_any = True\n",
    "                rows = votes_df[\n",
    "                    (votes_df[\"participant_id\"] == u) & (votes_df[\"group_id\"] == g)\n",
    "                ]\n",
    "                if not rows.empty:\n",
    "                    votes_for_topic.append(rows.iloc[0][\"vote\"])\n",
    "        if eligible_any:\n",
    "            elig_users += 1\n",
    "            if votes_for_topic:\n",
    "                voted_users += 1\n",
    "                # pick strongest vote for this topic\n",
    "                v = sorted(votes_for_topic, key=lambda x: (abs(x), x), reverse=True)[0]\n",
    "                if v == 1:\n",
    "                    agree_cnt += 1\n",
    "                elif v == -1:\n",
    "                    disagree_cnt += 1\n",
    "                else:\n",
    "                    pass_cnt += 1\n",
    "    coverage = voted_users / max(elig_users, 1)\n",
    "    total = agree_cnt + disagree_cnt + pass_cnt\n",
    "    if total > 0:\n",
    "        agree_pct = agree_cnt / total\n",
    "        disagree_pct = disagree_cnt / total\n",
    "        pass_pct = pass_cnt / total\n",
    "        polarity = (agree_cnt - disagree_cnt) / total\n",
    "        probs = np.array([agree_pct, pass_pct, disagree_pct])\n",
    "        entropy = -np.sum(probs * np.log(probs + 1e-12)) / np.log(3)\n",
    "    else:\n",
    "        agree_pct = disagree_pct = pass_pct = polarity = entropy = np.nan\n",
    "    return {\n",
    "        \"coverage\": coverage,\n",
    "        \"agree_pct\": agree_pct,\n",
    "        \"disagree_pct\": disagree_pct,\n",
    "        \"pass_pct\": pass_pct,\n",
    "        \"polarity\": polarity,\n",
    "        \"controversy\": entropy,\n",
    "    }\n",
    "\n",
    "\n",
    "topic_stats = {c: compute_topic_stats(c) for c in clusters}\n",
    "\n",
    "# Display summary per topic\n",
    "for c in clusters:\n",
    "    print(f\"Topic {c}\")\n",
    "    print(\"  Keywords:\", \", \".join(keywords_per_topic.get(c, [])))\n",
    "    print(\"  Representative sentences:\")\n",
    "    for s in rep_sentences.get(c, []):\n",
    "        print(\"   -\", s)\n",
    "    stats = topic_stats[c]\n",
    "    print(\n",
    "        \"  Coverage: {:.2%}, Agree: {:.1%}, Disagree: {:.1%}, Pass: {:.1%}, Polarity: {:.2f}, Controversy: {:.2f}\".format(\n",
    "            stats[\"coverage\"],\n",
    "            (\n",
    "                stats[\"agree_pct\"] * 100\n",
    "                if stats[\"agree_pct\"] == stats[\"agree_pct\"]\n",
    "                else float(\"nan\")\n",
    "            ),\n",
    "            (\n",
    "                stats[\"disagree_pct\"] * 100\n",
    "                if stats[\"disagree_pct\"] == stats[\"disagree_pct\"]\n",
    "                else float(\"nan\")\n",
    "            ),\n",
    "            (\n",
    "                stats[\"pass_pct\"] * 100\n",
    "                if stats[\"pass_pct\"] == stats[\"pass_pct\"]\n",
    "                else float(\"nan\")\n",
    "            ),\n",
    "            stats[\"polarity\"],\n",
    "            stats[\"controversy\"],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1950708b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated an end‑to‑end approach to analysing single‑sentence submissions with crowd‑sourced votes. We handled semantic redundancy via paraphrase mining, estimated exposure probabilities to correct for non‑uniform visibility, learned vote‑driven embeddings through signed matrix factorisation weighted by inverse propensities, fused them with semantic sentence embeddings, clustered the fused vectors to find topics, and labelled those topics with keywords and social statistics.\n",
    "\n",
    "The techniques showcased here are modular: you can swap out the embedding models, use alternative exposure models or matrix factorisation algorithms, or experiment with different clustering methods. The general principle remains: **model exposure**, **learn meaningful representations**, **cluster to discover structure**, and **provide interpretable summaries**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (jax)",
   "language": "python",
   "name": "jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
