    "# Load the data exported from pol.is. These files are required.\n",
    "if not (comments_path.exists() and votes_path.exists()):\n",
    "    raise FileNotFoundError(\n",
    "        \"comments.csv and votes.csv must be present from the pol.is export.\"\n",
    "\n",
    "comments_raw = pd.read_csv(comments_path)\n",
    "votes_raw = pd.read_csv(votes_path)\n",
    "\n",
    "# Normalise column names and timestamps\n",
    "sentences_df = comments_raw.rename(\n",
    "    columns={\n",
    "        \"comment-id\": \"sentence_id\",\n",
    "        \"comment-body\": \"text\",\n",
    "        \"author-id\": \"author_id\",\n",
    "    }\n",
    ").copy()\n",
    "sentences_df[\"timestamp\"] = pd.to_datetime(\n",
    "    sentences_df[\"timestamp\"], unit=\"s\", errors=\"coerce\"\n",
    ")\n",
    "sentences_df = sentences_df[\n",
    "    [\n",
    "        \"sentence_id\",\n",
    "        \"text\",\n",
    "        \"timestamp\",\n",
    "        \"author_id\",\n",
    "        \"agrees\",\n",
    "        \"disagrees\",\n",
    "        \"moderated\",\n",
    "]\n",
    "sentences_df.sort_values(\"timestamp\", inplace=True)\n",
    "votes_df = votes_raw.rename(\n",
    "    columns={\n",
    "        \"comment-id\": \"sentence_id\",\n",
    "        \"voter-id\": \"participant_id\",\n",
    "    }\n",
    ").copy()\n",
    "votes_df[\"timestamp\"] = pd.to_datetime(\n",
    "    votes_df[\"timestamp\"], unit=\"s\", errors=\"coerce\"\n",
    ")\n",
    "votes_df = votes_df[[\"participant_id\", \"sentence_id\", \"vote\", \"timestamp\"]]\n",
    "votes_df.sort_values([\"participant_id\", \"timestamp\"], inplace=True)\n",
    "print(f\"Loaded {len(sentences_df)} comments and {len(votes_df)} votes from pol.is export.\")\n",
    "# Import sentence-transformers (required for deduplication)\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer, util\n",
    "\n"
    "# Compute biâ€‘encoder embeddings\n",
    "bi_encoder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "sent_emb = bi_encoder.encode(\n",
    "    sentences_df[\"text\"].tolist(),\n",
    "    convert_to_tensor=True,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "candidates = util.paraphrase_mining_embeddings(sent_emb, top_k=20)\n",
    "COS_THRESHOLD = 0.9\n",
    "paraphrase_pairs = [(i, j) for score, i, j in candidates if score >= COS_THRESHOLD]\n",
    "    ce_model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "    texts = sentences_df[\"text\"].tolist()\n",
    "    pair_texts = [(texts[i], texts[j]) for i, j in paraphrase_pairs]\n",
    "    ce_scores = ce_model.predict(pair_texts)\n",
    "    CE_THRESHOLD = 0.50\n",
    "    verified_pairs = [\n",
    "        (i, j) for (i, j), s in zip(paraphrase_pairs, ce_scores) if s >= CE_THRESHOLD\n",
    "    ]\n",
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(len(sentences_df)))\n",
    "G.add_edges_from(verified_pairs)\n",
    "components = list(nx.connected_components(G))\n",
    "    sentences_df[[\"sentence_id\", \"group_id\"]], on=\"sentence_id\", how=\"left\",\n",
    "print(f\"Deduplication complete: {len(components)} groups identified.\")\n",
    "\n"
    "sentence_embeddings = bi_encoder.encode(\n",
    "    sentences_df[\"text\"].tolist(),\n",
    "    convert_to_tensor=False,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "group_lookup = {g: idx for idx, g in enumerate(groups)}\n",
    "group_vectors = np.zeros((len(groups), sentence_embeddings.shape[1]), dtype=float)\n",
    "counts = np.zeros(len(groups), dtype=int)\n",
    "for emb, gid in zip(sentence_embeddings, sentences_df[\"group_id\"].tolist()):\n",
    "    idx = group_lookup.get(gid)\n",
    "    if idx is None:\n",
    "        continue\n",
    "    group_vectors[idx] += emb\n",
    "    counts[idx] += 1\n",
    "nonzero = counts > 0\n",
    "if np.any(nonzero):\n",
    "    group_vectors[nonzero] /= counts[nonzero, None]\n",
    "text_embeddings = group_vectors if np.any(counts) else None\n",
    "cluster_method = \"hdbscan\"  # Options: \"hdbscan\" or \"kmeans\"\n",
    "if cluster_method == \"hdbscan\":\n",
    "        raise ValueError(\"HDBSCAN returned only noise.\")\n",
    "elif cluster_method == \"kmeans\":\n",
    "    n_samples = features_norm.shape[0]\n",
    "    K = min(max(2, n_samples // 3), n_samples)\n",
    "    if K < 1:\n",
    "        raise ValueError(\"Insufficient samples for k-means.\")\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported cluster_method: {cluster_method}\")\n",
    "    n_clusters=None, distance_threshold=0.5, metric=\"euclidean\", linkage=\"average\",\n",
    "sentences_df[[\"sentence_id\", \"group_id\", \"topic\", \"super_topic\"]].head()\n"
